{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_prep_dir = \"./prep_src\"\n",
    "os.makedirs(modules_dir, exist_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_prep_dir}/data_prep_functions.py\n",
    "            \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import logging\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory to save the plots\n",
    "plot_save_dir = 'plots'\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def check_missing_values(df, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Check and visualize missing values in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to check for missing values.\n",
    "        artifact_save_dir (str): Directory to save the heatmap plot and other artifacts.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # Check for missing values and compute the count of missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Plot a heatmap of missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "\n",
    "    # List the number of missing values in each column\n",
    "    log.info(\"Number of missing values in each column:\")\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            log.info(f\"Column '{column}' had {count} missing values.\")\n",
    "\n",
    "    # Create the artifact_save_dir directory if it doesn't exist\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "\n",
    "    # Save the heatmap plot as an image in the specified directory\n",
    "    plot_name = 'missing_values_heatmap'\n",
    "    plot_save_path = os.path.join(artifact_save_dir, f\"{plot_name}.png\")\n",
    "    plt.savefig(plot_save_path)\n",
    "    log.info(f'{plot_name} saved at: {plot_save_path}')\n",
    "\n",
    "    # Log the heatmap plot as an artifact using MLflow\n",
    "    mlflow.log_artifact(plot_save_path, artifact_path=f'{plot_name}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def replace_missing_values(df, ms_threshold: int, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Replace missing values in a DataFrame using interpolation and iterative imputation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing missing values.\n",
    "        ms_threshold (int): Threshold to switch between interpolation and iterative imputer.\n",
    "        artifact_save_dir (str, optional): Directory to save artifacts (e.g., logs) (default: None).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values replaced.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # If an artifact_save_dir is specified, configure the logger to save logs to that directory\n",
    "    if artifact_save_dir:\n",
    "        log_filename = 'replace_missing_values.log'\n",
    "        log_filepath = os.path.join(artifact_save_dir, log_filename)\n",
    "\n",
    "        # Configure the logger\n",
    "        logging.basicConfig(filename=log_filepath, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Threshold to switch between interpolation and iterative imputer\n",
    "    interpolation_threshold = ms_threshold\n",
    "\n",
    "    # Count the missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # List to store column names that need imputation\n",
    "    columns_to_impute = []\n",
    "\n",
    "    # Identify columns where the gap between missing values is less than the threshold\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            indices = df[column].index[df[column].isnull()]\n",
    "            differences = np.diff(indices)\n",
    "            if all(diff <= interpolation_threshold for diff in differences):\n",
    "                columns_to_impute.append(column)\n",
    "\n",
    "    # Separate columns for interpolation and iterative imputer\n",
    "    columns_to_interpolate = [col for col in columns_to_impute if col not in columns_to_impute]\n",
    "    columns_to_iterative_impute = [col for col in columns_to_impute if col in columns_to_impute]\n",
    "\n",
    "    # Replace missing values with interpolation\n",
    "    if len(columns_to_interpolate) > 0:\n",
    "        imputer = SimpleImputer(strategy='nearest')\n",
    "        df[columns_to_interpolate] = imputer.fit_transform(df[columns_to_interpolate])\n",
    "        for column in columns_to_interpolate:\n",
    "            log.info(f\"Imputed '{column}' using 'nearest' strategy.\")\n",
    "\n",
    "    # Replace missing values with iterative imputer\n",
    "    if len(columns_to_iterative_impute) > 0:\n",
    "        imputer = IterativeImputer()\n",
    "        df[columns_to_iterative_impute] = imputer.fit_transform(df[columns_to_iterative_impute])\n",
    "        for column in columns_to_iterative_impute:\n",
    "            log.info(f\"Imputed '{column}' using 'iterative' strategy.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on Spearman correlation coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the dataset.\n",
    "    - corr_threshold: The threshold for correlation above which features will be dropped (default is 0.8).\n",
    "    - plot_heatmaps: Whether to plot heatmaps before and after dropping (default is True).\n",
    "    - artifact_save_dir: Directory to save the correlation heatmap plots (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the highly correlated features dropped.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    if artifact_save_dir and not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Calculate the correlation matrix (Spearman by default in pandas)\n",
    "    corr_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Plot the correlation heatmap before dropping\n",
    "        fig_before = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (Before Dropping)\")\n",
    "        sns_plot_before = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            before_plot_path = os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\")\n",
    "            plt.savefig(before_plot_path)\n",
    "            log.info(\"Correlation heatmap (Before Dropping): %s\", before_plot_path)\n",
    "            \n",
    "        mlflow.log_artifact(before_plot_path, artifact_path=\"correlation_heatmap_before.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a set to store the columns to drop\n",
    "    columns_to_drop = set()\n",
    "    \n",
    "    # Create a list to store the names of the dropped columns\n",
    "    dropped_columns = []\n",
    "    \n",
    "    # Iterate through the columns and identify highly correlated features\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            if col1 != col2 and abs(corr_matrix.loc[col1, col2]) >= corr_threshold:\n",
    "                # Check if col1 or col2 should be dropped based on their mean correlation\n",
    "                mean_corr_col1 = corr_matrix.loc[col1, :].drop(col1).abs().mean()\n",
    "                mean_corr_col2 = corr_matrix.loc[col2, :].drop(col2).abs().mean()\n",
    "                \n",
    "                if mean_corr_col1 > mean_corr_col2:\n",
    "                    columns_to_drop.add(col1)\n",
    "                    dropped_columns.append(col1)\n",
    "                else:\n",
    "                    columns_to_drop.add(col2)\n",
    "                    dropped_columns.append(col2)\n",
    "    \n",
    "    # Drop the highly correlated features from the DataFrame\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Calculate the correlation matrix after dropping\n",
    "        corr_matrix_after_drop = df.corr(method='spearman')\n",
    "        \n",
    "        # Plot the correlation heatmap after dropping\n",
    "        fig_after = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (After Dropping)\")\n",
    "        sns_plot_after = sns.heatmap(corr_matrix_after_drop, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            after_plot_path = os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\")\n",
    "            plt.savefig(after_plot_path)\n",
    "            log.info(\"Correlation heatmap (After Dropping): %s\", after_plot_path)\n",
    "            \n",
    "        mlflow.log_artifact(after_plot_path, artifact_path=\"correlation_heatmap_after.png\")\n",
    "        plt.show()\n",
    "           \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"Dropped columns: %s\", dropped_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_features(df, max_unique_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Drop high cardinality features (columns) from a DataFrame based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        max_unique_threshold (float): The maximum allowed fraction of unique values in a column (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with high cardinality columns dropped.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"Input DataFrame 'df' cannot be None.\")\n",
    "        \n",
    "    # Calculate the maximum number of allowed unique values for each column\n",
    "    max_unique_values = len(df) * max_unique_threshold\n",
    "    \n",
    "    # Identify and drop columns with unique values exceeding the threshold\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > max_unique_values]\n",
    "    \n",
    "    # Log the names of the dropped columns using MLflow\n",
    "    if high_cardinality_columns:\n",
    "        mlflow.log_param(\"HighCardinalityColumns\", ', '.join(high_cardinality_columns))\n",
    "    \n",
    "    df_dropped = df.drop(columns=high_cardinality_columns)\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "def select_categorical_columns(data):\n",
    "    \"\"\"\n",
    "    Select categorical columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - A list of column names that are categorical.\n",
    "    \"\"\"\n",
    "    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return categorical_columns\n",
    "\n",
    "def custom_train_test_split(data, target_column, test_size=0.2, random_state=101, time_series=False):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "    - target_column: Name of the target column.\n",
    "    - test_size: Proportion of the dataset to include in the test split (default is 0.2).\n",
    "    - random_state: Seed for random number generation (optional).\n",
    "    - time_series: Set to True if the data is time series data (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: The split datasets.\n",
    "    \"\"\"\n",
    "    if time_series:\n",
    "        # For time series data, split by a specific time point\n",
    "        data = data.sort_index()  # Sort by time index if not already sorted\n",
    "        n = len(data)\n",
    "        split_index = int((1 - test_size) * n)\n",
    "        X_train, X_test = data.iloc[:split_index, :-1], data.iloc[split_index:, :-1]\n",
    "        y_train, y_test = data.iloc[:split_index][target_column], data.iloc[split_index:][target_column]\n",
    "    else:\n",
    "        # For regular (cross-sectional) data, use train_test_split\n",
    "        X = data.drop(columns=[target_column])\n",
    "        y = data[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_prep_dir = \"./train_src\"\n",
    "os.makedirs(modules_dir, exist_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29037495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {train_data_dir}/training_functions.py\n",
    "%%writefile {modules_dir}/tune_train_test.py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def custom_train_test_split(data, target_column, test_size=0.2, random_state=101, time_series=False):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "    - target_column: Name of the target column.\n",
    "    - test_size: Proportion of the dataset to include in the test split (default is 0.2).\n",
    "    - random_state: Seed for random number generation (optional).\n",
    "    - time_series: Set to True if the data is time series data (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: The split datasets.\n",
    "    \"\"\"\n",
    "    if time_series:\n",
    "        # For time series data, split by a specific time point\n",
    "        data = data.sort_index()  # Sort by time index if not already sorted\n",
    "        n = len(data)\n",
    "        split_index = int((1 - test_size) * n)\n",
    "        X_train, X_test = data.iloc[:split_index, :-1], data.iloc[split_index:, :-1]\n",
    "        y_train, y_test = data.iloc[:split_index][target_column], data.iloc[split_index:][target_column]\n",
    "    else:\n",
    "        # For regular (cross-sectional) data, use train_test_split\n",
    "        X = data.drop(columns=[target_column])\n",
    "        y = data[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, model_prefix:str, param_grid=None, random_search=False, bayesian_search=False, n_iter=10, random_seed=101):\n",
    "    \"\"\"\n",
    "    Train a Histogram Gradient Boosting Classifier and tune its hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_test, y_test: Testing data and labels.\n",
    "    - model_prefix: Prefix for model artifacts.\n",
    "    - param_grid: Hyperparameter grid to search (default is None).\n",
    "    - random_search: Whether to use random search instead of grid search (default is False).\n",
    "    - bayesian_search: Whether to use Bayesian hyperparameter search (default is False).\n",
    "    - n_iter: Number of parameter settings that are sampled (only for random_search or bayesian_search).\n",
    "\n",
    "    Returns:\n",
    "    - Trained model, best hyperparameters, and test accuracy.\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    # Create a Histogram Gradient Boosting Classifier\n",
    "    clf = HistGradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Combine preprocessing and classifier into a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    if not bayesian_search:\n",
    "        # Define hyperparameters for grid search or random search\n",
    "        hyperparameters = {\n",
    "            'classifier__max_iter': [100, 200, 300],  # Adjust the values as needed\n",
    "            'classifier__learning_rate': [0.001, 0.01, 0.1],  # Adjust the values as needed\n",
    "            'classifier__max_depth': [3, 4, 5],  # Adjust the values as needed\n",
    "            'classifier__l2_regularization': [0.0, 0.1, 0.2]  # Adjust the values as needed\n",
    "        }\n",
    "\n",
    "        if random_search:\n",
    "            # Use RandomizedSearchCV\n",
    "            search = RandomizedSearchCV(pipeline, param_distributions=hyperparameters, n_iter=n_iter, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "        else:\n",
    "            # Use GridSearchCV\n",
    "            search = GridSearchCV(pipeline, param_grid=hyperparameters, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "    else:\n",
    "        # Use Bayesian hyperparameter search with BayesSearchCV\n",
    "        param_grid = {\n",
    "            'classifier__max_iter': (100, 300),\n",
    "            'classifier__learning_rate': (0.001, 0.1),\n",
    "            'classifier__max_depth': (3, 5),\n",
    "            'classifier__l2_regularization': (0.0, 0.2)\n",
    "        }\n",
    "\n",
    "        search = BayesSearchCV(pipeline, param_grid, n_iter=n_iter, cv=TimeSeriesSplit(n_splits=3), scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "\n",
    "    # Fit the search to the training data\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and the best estimator (trained model)\n",
    "    best_params = search.best_params_\n",
    "    best_estimator = search.best_estimator_\n",
    "    \n",
    "    log.info('Parameters chosen are:')\n",
    "    log.info(best_params)\n",
    "    \n",
    "    log.info('The best estimator is:')\n",
    "    log.info(best_estimator)\n",
    "    \n",
    "    # Evaluate the best model on the test data\n",
    "   # y_pred = best_estimator.predict(X_test)\n",
    "   # test_accuracy = accuracy_score(y_test, y_pred)\n",
    "   # log.info(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "    \n",
    "    # Save the best model to a file\n",
    "    model_filename = f'{model_prefix}_best_model.joblib'\n",
    "    joblib.dump(best_estimator, model_filename)\n",
    "    \n",
    "    # Save best hyperparameters to a JSON file\n",
    "    hyperparameters_filename = f'{model_prefix}_hyperparameters.json'\n",
    "    log.info(f'Saving best hyperparameters for {model_prefix} as {hyperparameters_filename}')\n",
    "    with open(hyperparameters_filename, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "        \n",
    "    return best_params, hyperparameters_filename\n",
    "\n",
    "def train_model(X_train, y_train, model_name:str, hyperparam: dict=None, hyperparam_filename: str=None):\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    if hyperparam_filename is not None:\n",
    "        log.info(f'Loading in hyperparameters: {hyperparam_filename}')\n",
    "        with open(hyperparam_filename, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "    elif hyperparam is not None:\n",
    "        best_params = hyperparam\n",
    "    else:\n",
    "        raise ValueError('Either hyperparam or hyperparam_filename must be assigned')\n",
    "    \n",
    "    # Create and train the model with the specified hyperparameters\n",
    "    log.info('Training Model')\n",
    "    trained_model = HistGradientBoostingClassifier(class_weight='balanced',\n",
    "        max_iter=best_params['classifier__max_iter'],\n",
    "        learning_rate=best_params['classifier__learning_rate'],\n",
    "        max_depth=best_params['classifier__max_depth'],\n",
    "        l2_regularization=best_params['classifier__l2_regularization'],\n",
    "        random_state=10\n",
    "    )\n",
    "    trained_model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    log.info(f'Saving {model_name}')\n",
    "    joblib.dump(trained_model, model_name)\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "def predict_model(trained_model, X_test, inference_col_name):\n",
    "    \"\"\"\n",
    "    Predict using a trained machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_model: The trained machine learning model.\n",
    "    - X_test: The test dataset on which to make predictions.\n",
    "    - inference_col_name: The name of the column to store predictions in the inference DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - inference_df: The DataFrame containing predictions.\n",
    "    - inference_col_name: The name of the column where predictions are stored.\n",
    "    - predictions: The predictions made by the model.\n",
    "    \"\"\"\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "    \n",
    "    # Get the one-hot encoded feature names\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    cat_feature_names = list(ohe.get_feature_names_out(input_features=categorical_features))\n",
    "    \n",
    "    # Combine the one-hot encoded feature names and non-categorical column names\n",
    "    all_column_names = cat_feature_names + list(X_test.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    \n",
    "    # Convert X_test_transformed to a DataFrame with appropriate column names\n",
    "    inference_df = pd.DataFrame(X_test_transformed, columns=all_column_names)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predictions = trained_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Add predictions to the DataFrame with the specified column name\n",
    "    inference_df[inference_col_name] = predictions\n",
    "    \n",
    "    return inference_df, inference_col_name, predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e11a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_functions.py\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_HGboost_classification_model\n",
    "display_name: HistGradientBoostingModel\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  training_data: \n",
    "    type: uri_folder\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "  test_data:\n",
    "    type: uri_folder\n",
    "code: ./train_src\n",
    "environment: azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python train.py \n",
    "  --training_data ${{inputs.training_data}} \n",
    "  --test_data ${{outputs.test_data}} \n",
    "  --model_output ${{outputs.model_output}}\n",
    "# </component>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
