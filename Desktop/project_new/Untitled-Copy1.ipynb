{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b494a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "1e4ce9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modules_dir = \"./modules\"\n",
    "os.makedirs(modules_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466bb32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_prep.py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory to save the plots\n",
    "plot_save_dir = 'plots'\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def check_missing_values(df, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Check and visualize missing values in a DataFra: str=me.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to check for missing values.\n",
    "        artifact_save_dir (str): Directory to save the heatmap plot and other artifacts.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # Check for missing values and compute the count of missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Plot a heatmap of missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "\n",
    "    # List the number of missing values in each column\n",
    "    log.info(\"Number of missing values in each column:\")\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            log.info(f\"Column '{column}' had {count} missing values.\")\n",
    "\n",
    "    # Create the artifact_save_dir directory if it doesn't exist\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "\n",
    "    # Save the heatmap plot as an image in the specified directory\n",
    "    plot_name = 'missing_values_heatmap'\n",
    "    plot_save_path = os.path.join(artifact_save_dir, f\"{plot_name}.png\")\n",
    "    plt.savefig(plot_save_path)\n",
    "    log.info(f'{plot_name} saved at: {plot_save_path}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def replace_missing_values(df, ms_threshold: int, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Replace missing values in a DataFrame using interpolation and iterative imputation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing missing values.\n",
    "        ms_threshold (int): Threshold to switch between interpolation and iterative imputer.\n",
    "        artifact_save_dir (str, optional): Directory to save artifacts (e.g., logs) (default: None).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values replaced.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # If an artifact_save_dir is specified, configure the logger to save logs to that directory\n",
    "    if artifact_save_dir:\n",
    "        log_filename = 'replace_missing_values.log'\n",
    "        log_filepath = os.path.join(artifact_save_dir, log_filename)\n",
    "\n",
    "        # Configure the logger\n",
    "        logging.basicConfig(filename=log_filepath, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Threshold to switch between interpolation and iterative imputer\n",
    "    interpolation_threshold = ms_threshold\n",
    "\n",
    "    # Count the missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # List to store column names that need imputation\n",
    "    columns_to_impute = []\n",
    "\n",
    "    # Identify columns where the gap between missing values is less than the threshold\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            indices = df[column].index[df[column].isnull()]\n",
    "            differences = np.diff(indices)\n",
    "            if all(diff <= interpolation_threshold for diff in differences):\n",
    "                columns_to_impute.append(column)\n",
    "\n",
    "    # Separate columns for interpolation and iterative imputer\n",
    "    columns_to_interpolate = [col for col in columns_to_impute if col not in columns_to_impute]\n",
    "    columns_to_iterative_impute = [col for col in columns_to_impute if col in columns_to_impute]\n",
    "\n",
    "    # Replace missing values with interpolation\n",
    "    if len(columns_to_interpolate) > 0:\n",
    "        imputer = SimpleImputer(strategy='nearest')\n",
    "        df[columns_to_interpolate] = imputer.fit_transform(df[columns_to_interpolate])\n",
    "        for column in columns_to_interpolate:\n",
    "            log.info(f\"Imputed '{column}' using 'nearest' strategy.\")\n",
    "\n",
    "    # Replace missing values with iterative imputer\n",
    "    if len(columns_to_iterative_impute) > 0:\n",
    "        imputer = IterativeImputer()\n",
    "        df[columns_to_iterative_impute] = imputer.fit_transform(df[columns_to_iterative_impute])\n",
    "        for column in columns_to_iterative_impute:\n",
    "            log.info(f\"Imputed '{column}' using 'iterative' strategy.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on Spearman correlation coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the dataset.\n",
    "    - corr_threshold: The threshold for correlation above which features will be dropped (default is 0.8).\n",
    "    - plot_heatmaps: Whether to plot heatmaps before and after dropping (default is True).\n",
    "    - artifact_save_dir: Directory to save the correlation heatmap plots (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the highly correlated features dropped.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    if artifact_save_dir and not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Calculate the correlation matrix (Spearman by default in pandas)\n",
    "    corr_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Plot the correlation heatmap before dropping\n",
    "        fig_before = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (Before Dropping)\")\n",
    "        sns_plot_before = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "            log.info(\"Correlation heatmap (Before Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Create a set to store the columns to drop\n",
    "    columns_to_drop = set()\n",
    "    \n",
    "    # Create a list to store the names of the dropped columns\n",
    "    dropped_columns = []\n",
    "    \n",
    "    # Iterate through the columns and identify highly correlated features\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            if col1 != col2 and abs(corr_matrix.loc[col1, col2]) >= corr_threshold:\n",
    "                # Check if col1 or col2 should be dropped based on their mean correlation\n",
    "                mean_corr_col1 = corr_matrix.loc[col1, :].drop(col1).abs().mean()\n",
    "                mean_corr_col2 = corr_matrix.loc[col2, :].drop(col2).abs().mean()\n",
    "                \n",
    "                if mean_corr_col1 > mean_corr_col2:\n",
    "                    columns_to_drop.add(col1)\n",
    "                    dropped_columns.append(col1)\n",
    "                else:\n",
    "                    columns_to_drop.add(col2)\n",
    "                    dropped_columns.append(col2)\n",
    "    \n",
    "    # Drop the highly correlated features from the DataFrame\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Calculate the correlation matrix after dropping\n",
    "        corr_matrix_after_drop = df.corr(method='spearman')\n",
    "        \n",
    "        # Plot the correlation heatmap after dropping\n",
    "        fig_after = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (After Dropping)\")\n",
    "        sns_plot_after = sns.heatmap(corr_matrix_after_drop, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "            log.info(\"Correlation heatmap (After Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"Dropped columns: %s\", dropped_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_columns(data, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drop selected columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "    - columns_to_drop: Single column name or a list of column names to be dropped.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    if isinstance(columns_to_drop, str):\n",
    "        # If a single column name is provided, convert it to a list\n",
    "        columns_to_drop = [columns_to_drop]\n",
    "\n",
    "    # Drop the specified columns from the DataFrame\n",
    "    df = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"bad_columns_dropped are/is: %s\", columns_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_features(df, max_unique_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Drop high cardinality features (columns) from a DataFrame based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        max_unique_threshold (float): The maximum allowed fraction of unique values in a column (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with high cardinality columns dropped.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"Input DataFrame 'df' cannot be None.\")\n",
    "        \n",
    "    # Calculate the maximum number of allowed unique values for each column\n",
    "    max_unique_values = len(df) * max_unique_threshold\n",
    "    \n",
    "    # Identify and drop columns with unique values exceeding the threshold\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > max_unique_values]\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    if high_cardinality_columns:\n",
    "        log.info(f\"Dropped high cardinality columns: {', '.join(high_cardinality_columns)}\")\n",
    "    \n",
    "    df_dropped = df.drop(columns=high_cardinality_columns)\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "def select_categorical_columns(data):\n",
    "    \"\"\"\n",
    "    Select categorical columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - A list of column names that are categorical.\n",
    "    \"\"\"\n",
    "    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return categorical_columns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a7c9664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./src/modules\")\n",
    "import data_prep_functions as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af74f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (23.9.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.3.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.3.2)\n",
      "Requirement already satisfied: PyYAML in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "c1d53bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5a92dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/tune_train_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/tune_train_test.py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def custom_train_test_split(data, target_column, test_size=0.2, random_state=101, time_series=False):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "    - target_column: Name of the target column.\n",
    "    - test_size: Proportion of the dataset to include in the test split (default is 0.2).\n",
    "    - random_state: Seed for random number generation (optional).\n",
    "    - time_series: Set to True if the data is time series data (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: The split datasets.\n",
    "    \"\"\"\n",
    "    if time_series:\n",
    "        # For time series data, split by a specific time point\n",
    "        data = data.sort_index()  # Sort by time index if not already sorted\n",
    "        n = len(data)\n",
    "        split_index = int((1 - test_size) * n)\n",
    "        X_train, X_test = data.iloc[:split_index, :-1], data.iloc[split_index:, :-1]\n",
    "        y_train, y_test = data.iloc[:split_index][target_column], data.iloc[split_index:][target_column]\n",
    "    else:\n",
    "        # For regular (cross-sectional) data, use train_test_split\n",
    "        X = data.drop(columns=[target_column])\n",
    "        y = data[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, model_prefix:str, param_grid=None, random_search=False, bayesian_search=False, n_iter=10, random_seed=101):\n",
    "    \"\"\"\n",
    "    Train a Histogram Gradient Boosting Classifier and tune its hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_test, y_test: Testing data and labels.\n",
    "    - model_prefix: Prefix for model artifacts.\n",
    "    - param_grid: Hyperparameter grid to search (default is None).\n",
    "    - random_search: Whether to use random search instead of grid search (default is False).\n",
    "    - bayesian_search: Whether to use Bayesian hyperparameter search (default is False).\n",
    "    - n_iter: Number of parameter settings that are sampled (only for random_search or bayesian_search).\n",
    "\n",
    "    Returns:\n",
    "    - Trained model, best hyperparameters, and test accuracy.\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    # Create a Histogram Gradient Boosting Classifier\n",
    "    clf = HistGradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Combine preprocessing and classifier into a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    if not bayesian_search:\n",
    "        # Define hyperparameters for grid search or random search\n",
    "        hyperparameters = {\n",
    "            'classifier__max_iter': [100, 200, 300],  # Adjust the values as needed\n",
    "            'classifier__learning_rate': [0.001, 0.01, 0.1],  # Adjust the values as needed\n",
    "            'classifier__max_depth': [3, 4, 5],  # Adjust the values as needed\n",
    "            'classifier__l2_regularization': [0.0, 0.1, 0.2]  # Adjust the values as needed\n",
    "        }\n",
    "\n",
    "        if random_search:\n",
    "            # Use RandomizedSearchCV\n",
    "            search = RandomizedSearchCV(pipeline, param_distributions=hyperparameters, n_iter=n_iter, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "        else:\n",
    "            # Use GridSearchCV\n",
    "            search = GridSearchCV(pipeline, param_grid=hyperparameters, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "    else:\n",
    "        # Use Bayesian hyperparameter search with BayesSearchCV\n",
    "        param_grid = {\n",
    "            'classifier__max_iter': (100, 300),\n",
    "            'classifier__learning_rate': (0.001, 0.1),\n",
    "            'classifier__max_depth': (3, 5),\n",
    "            'classifier__l2_regularization': (0.0, 0.2)\n",
    "        }\n",
    "\n",
    "        search = BayesSearchCV(pipeline, param_grid, n_iter=n_iter, cv=TimeSeriesSplit(n_splits=3), scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "\n",
    "    # Fit the search to the training data\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and the best estimator (trained model)\n",
    "    best_params = search.best_params_\n",
    "    best_estimator = search.best_estimator_\n",
    "    \n",
    "    log.info('Parameters chosen are:')\n",
    "    log.info(best_params)\n",
    "    \n",
    "    log.info('The best estimator is:')\n",
    "    log.info(best_estimator)\n",
    "    \n",
    "    # Evaluate the best model on the test data\n",
    "   # y_pred = best_estimator.predict(X_test)\n",
    "   # test_accuracy = accuracy_score(y_test, y_pred)\n",
    "   # log.info(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "    \n",
    "    # Save the best model to a file\n",
    "    model_filename = f'{model_prefix}_best_model.joblib'\n",
    "    joblib.dump(best_estimator, model_filename)\n",
    "    \n",
    "    # Save best hyperparameters to a JSON file\n",
    "    hyperparameters_filename = f'{model_prefix}_hyperparameters.json'\n",
    "    log.info(f'Saving best hyperparameters for {model_prefix} as {hyperparameters_filename}')\n",
    "    with open(hyperparameters_filename, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "        \n",
    "    return best_params, hyperparameters_filename\n",
    "\n",
    "def train_model(X_train, y_train, model_name:str, hyperparam: dict=None, hyperparam_filename: str=None):\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    if hyperparam_filename is not None:\n",
    "        log.info(f'Loading in hyperparameters: {hyperparam_filename}')\n",
    "        with open(hyperparam_filename, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "    elif hyperparam is not None:\n",
    "        best_params = hyperparam\n",
    "    else:\n",
    "        raise ValueError('Either hyperparam or hyperparam_filename must be assigned')\n",
    "    \n",
    "    # Create and train the model with the specified hyperparameters\n",
    "    log.info('Training Model')\n",
    "    trained_model = HistGradientBoostingClassifier(class_weight='balanced',\n",
    "        max_iter=best_params['classifier__max_iter'],\n",
    "        learning_rate=best_params['classifier__learning_rate'],\n",
    "        max_depth=best_params['classifier__max_depth'],\n",
    "        l2_regularization=best_params['classifier__l2_regularization'],\n",
    "        random_state=10\n",
    "    )\n",
    "    trained_model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    log.info(f'Saving {model_name}')\n",
    "    joblib.dump(trained_model, model_name)\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "def predict_model(trained_model, X_test, inference_col_name):\n",
    "    \"\"\"\n",
    "    Predict using a trained machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_model: The trained machine learning model.\n",
    "    - X_test: The test dataset on which to make predictions.\n",
    "    - inference_col_name: The name of the column to store predictions in the inference DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - inference_df: The DataFrame containing predictions.\n",
    "    - inference_col_name: The name of the column where predictions are stored.\n",
    "    - predictions: The predictions made by the model.\n",
    "    \"\"\"\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "    \n",
    "    # Get the one-hot encoded feature names\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    cat_feature_names = list(ohe.get_feature_names_out(input_features=categorical_features))\n",
    "    \n",
    "    # Combine the one-hot encoded feature names and non-categorical column names\n",
    "    all_column_names = cat_feature_names + list(X_test.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    \n",
    "    # Convert X_test_transformed to a DataFrame with appropriate column names\n",
    "    inference_df = pd.DataFrame(X_test_transformed, columns=all_column_names)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predictions = trained_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Add predictions to the DataFrame with the specified column name\n",
    "    inference_df[inference_col_name] = predictions\n",
    "    \n",
    "    return inference_df, inference_col_name, predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e2902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34876284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/evaluation.py\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Directory to save the plots\n",
    "plot_save_dir = 'plots'\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          model_name: str,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          artifact_save_dir: str):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix.\n",
    "\n",
    "    :param y_true: True labels of the data.\n",
    "    :param y_pred: Predicted labels of the data.\n",
    "    :param classes: List of class labels (e.g., ['Class 0', 'Class 1']).\n",
    "    :param model_name: Name of the model for plot naming.\n",
    "    :param normalize: If True, normalize the confusion matrix.\n",
    "    :param title: Title of the plot.\n",
    "    :param cmap: Colormap for the plot.\n",
    "    :param artifact_save_dir: Directory where artifacts including plots will be saved.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "\n",
    "    # Create figure and axis\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    # Customize plot labels and appearance\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Artifact save directory for plots\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Save the heatmap plot as an image\n",
    "    plot_name = f'{model_name}_confusion_matrix'  # Set the desired plot name\n",
    "    plot_save_path = os.path.join(artifact_save_dir, f\"{plot_name}.png\")  # Format the file name\n",
    "    plt.savefig(plot_save_path)\n",
    "    log.info(f'{plot_name} saved')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shap_feature_importance(X_test, model_name, n_features, artifact_save_dir: str):\n",
    "    \n",
    "    model_prefix = os.path.splitext(model_name)[0]\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        log.info(f'Creating {artifact_save_dir} directory in {os.getcwd()}')\n",
    "        os.makedirs(artifact_save_dir)\n",
    "        \n",
    "    log.info(f'Loading {model_name}')\n",
    "    model = joblib.load(model_name)\n",
    "    \n",
    "    log.info('Generating SHAP Values')\n",
    "    \n",
    "    # Convert X_test to a pandas DataFrame\n",
    "    X_test_df = pd.DataFrame(X_test)  # Assuming X_test is a 2D array\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test_df.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test_df)\n",
    "    \n",
    "    # Generate SHAP values using the transformed data\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_transformed)\n",
    "    \n",
    "    log.info('Saving SHAP Plot to Artifact Directory')\n",
    "    plt.clf()\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)) +\n",
    "        list(X_test_df.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    )\n",
    "    shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    # Artifact save directory for plots\n",
    "    plot_save_dir = os.path.join(artifact_save_dir, 'shap_summary_plots')\n",
    "    if not os.path.exists(plot_save_dir):\n",
    "        os.makedirs(plot_save_dir)\n",
    "    \n",
    "    plot_name = os.path.join(plot_save_dir, f'{model_prefix}_SHAP_summary.png')\n",
    "    plt.savefig(plot_name)\n",
    "    log.info(f'{plot_name} saved.')\n",
    "    \n",
    "    log.info(f'Extracting Top {n_features} Important features for the model')\n",
    "    feature_importances = np.abs(shap_values).mean(axis=0)\n",
    "    feature_importances_dict = dict(zip(feature_names, feature_importances))\n",
    "    sorted_features = sorted(feature_importances_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    log.info(f'Top {n_features} Important features are:')\n",
    "    top_n_feature = sorted_features[:n_features]\n",
    "    top_n_feature_names = [feature[0] for feature in top_n_feature]  # Extract feature names\n",
    "    top_n_feature_importances = [feature[1] for feature in top_n_feature]  # Extract importances\n",
    "    \n",
    "    log.info(', '.join([f'{feature}: {importance}' for feature, importance in top_n_feature]))\n",
    "    \n",
    "    return plot_name, shap_values, top_n_feature_names, top_n_feature_importances\n",
    "\n",
    "\n",
    "def shap_dependence_plots(X_test: pd.DataFrame, features: list, model_name: str, shap_values: np.array, artifact_save_dir: str):\n",
    "    model_prefix = os.path.splitext(model_name)[0]\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        log.info(f'Creating {artifact_save_dir} directory in {os.getcwd()}')\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    log.info(f'Loading {model_name}')\n",
    "    model = joblib.load(model_name)\n",
    "    \n",
    "    log.info('Generating SHAP Values')\n",
    "    \n",
    "    # Convert X_test to a pandas DataFrame\n",
    "    X_test_df = pd.DataFrame(X_test)  # Assuming X_test is a 2D array\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test_df.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test_df)\n",
    "    \n",
    "    # Generate feature names\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)) +\n",
    "        list(X_test_df.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    )\n",
    "\n",
    "    for index, feature_name in enumerate(features):\n",
    "        # Find the corresponding integer index for the feature name\n",
    "        try:\n",
    "            feature_index = feature_names.index(feature_name)\n",
    "        except ValueError:\n",
    "            log.warning(f'Feature {feature_name} not found in feature_names. Skipping.')\n",
    "            continue\n",
    "            \n",
    "        # Artifact save directory for plots\n",
    "        plot_save_dir = os.path.join(artifact_save_dir, 'shap_dependence_plots')\n",
    "        if not os.path.exists(plot_save_dir):\n",
    "            os.makedirs(plot_save_dir)\n",
    "        \n",
    "        plot_name = os.path.join(plot_save_dir, f'{model_prefix}_feature{index}_SHAP_dependence.png') \n",
    "        \n",
    "        # Clear the previous plot\n",
    "        plt.clf()\n",
    "        shap.dependence_plot(feature_index, shap_values, X_test_transformed, \n",
    "                             feature_names=feature_names, show=False)  # Pass feature names for labeling\n",
    "        # Save the SHAP dependence plot as an image\n",
    "        fig = plt.gcf()\n",
    "        plt.savefig(plot_name, dpi=150, bbox_inches='tight')\n",
    "        log.info(f'{plot_name} saved.')\n",
    "        \n",
    "    log.info(f'All SHAP dependence plots saved in {plot_save_dir}')\n",
    "    \n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def evaluate_classification_models(model_name, predictions, true_labels, target_names, plot_classification_report=False, artifact_save_dir: str = 'artifacts'):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model and generate a classification report.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model for plot naming.\n",
    "        predictions (array-like): Model predictions (predicted class labels).\n",
    "        true_labels (array-like): True class labels.\n",
    "        target_names (list): e.g ['Class 0', 'Class 1']\n",
    "        plot_classification_report (bool): Whether to plot the classification report (default: False).\n",
    "        artifact_save_dir (str): Directory where artifacts including plots will be saved.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed metrics.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    evaluation_results['accuracy'] = accuracy\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    evaluation_results['precision'] = precision\n",
    "\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    evaluation_results['recall'] = recall\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    evaluation_results['f1'] = f1\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels, predictions)\n",
    "        evaluation_results['roc_auc'] = roc_auc\n",
    "    except ValueError:\n",
    "        # roc_auc_score may not work for multiclass classification\n",
    "        evaluation_results['roc_auc'] = None\n",
    "\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    evaluation_results['confusion_matrix'] = confusion\n",
    "\n",
    "    if plot_classification_report:\n",
    "        # Generate the classification report\n",
    "        report = classification_report(\n",
    "            true_labels, predictions, target_names=target_names, output_dict=True\n",
    "        )\n",
    "\n",
    "        # Convert the classification report to a DataFrame for easy plotting\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "        # Plot the classification report as a heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(report_df.iloc[:-3, :-1], annot=True, cmap='Blues', fmt=\".2f\", cbar=False)\n",
    "        plt.title('Classification Report')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Metrics')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        model_prefix = os.path.splitext(model_name)[0]\n",
    "        plot_name = os.path.join(artifact_save_dir, f'{model_prefix}_Classification_Report.png')\n",
    "        plt.savefig(plot_name)\n",
    "        log.info(f'{plot_name} saved.')\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "        # Add classification report metrics to the evaluation results\n",
    "        for metric, values in report.items():\n",
    "            if isinstance(values, dict):\n",
    "                for class_name, value in values.items():\n",
    "                    metric_name = f\"{metric}_{class_name}\"\n",
    "                    evaluation_results[metric_name] = value\n",
    "    log.info(evaluation_results)\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "25303680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azureml-sdk in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (1.48.0)\n",
      "Collecting azureml-sdk\n",
      "  Downloading azureml_sdk-1.53.0-py3-none-any.whl (2.7 kB)\n",
      "Collecting azureml-train-automl-client~=1.53.0\n",
      "  Downloading azureml_train_automl_client-1.53.0-py3-none-any.whl (137 kB)\n",
      "     |████████████████████████████████| 137 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-dataset-runtime[fuse]~=1.53.0\n",
      "  Downloading azureml_dataset_runtime-1.53.0-py3-none-any.whl (2.3 kB)\n",
      "Collecting azureml-train-core~=1.53.0\n",
      "  Downloading azureml_train_core-1.53.0-py3-none-any.whl (8.6 MB)\n",
      "     |████████████████████████████████| 8.6 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-core~=1.53.0\n",
      "  Downloading azureml_core-1.53.0-py3-none-any.whl (3.3 MB)\n",
      "     |████████████████████████████████| 3.3 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-pipeline~=1.53.0\n",
      "  Downloading azureml_pipeline-1.53.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pkginfo in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.2.7)\n",
      "Requirement already satisfied: msrest<=0.7.1,>=0.5.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.7.1)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.4.8)\n",
      "Requirement already satisfied: SecretStorage<4.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.3.3)\n",
      "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.5.1)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.1.28)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.1.0)\n",
      "Collecting azure-mgmt-network==21.0.1\n",
      "  Downloading azure_mgmt_network-21.0.1-py3-none-any.whl (8.9 MB)\n",
      "     |████████████████████████████████| 8.9 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: azure-core<2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.29.2)\n",
      "Requirement already satisfied: azure-mgmt-storage<=21.0.0,>=16.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (20.1.0)\n",
      "Requirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.26.0)\n",
      "Requirement already satisfied: docker<7.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (6.0.1)\n",
      "Requirement already satisfied: argcomplete<3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied: paramiko<4.0.0,>=2.0.8 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.12.0)\n",
      "Requirement already satisfied: contextlib2<22.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.23 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.26.7)\n",
      "Requirement already satisfied: pyopenssl<24.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.0.0)\n",
      "Requirement already satisfied: knack~=0.10.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.10.1)\n",
      "Requirement already satisfied: humanfriendly<11.0,>=4.7 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.0)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<11.0.0,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.1.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization<4,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.0.0)\n",
      "Requirement already satisfied: pytz in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2021.3)\n",
      "Requirement already satisfied: msal-extensions<=1.0.0,>=0.3.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0.0)\n",
      "Requirement already satisfied: packaging<=23.0,>=20.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.3)\n",
      "Requirement already satisfied: azure-mgmt-resource<=22.0.0,>=15.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.2.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.8.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.15.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.21.0)\n",
      "Requirement already satisfied: pathspec<1.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.7.0)\n",
      "Requirement already satisfied: backports.tempfile in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry<11,>=8.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0.1)\n",
      "Requirement already satisfied: jsonpickle<4.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.2.0)\n",
      "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.6.4)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.61.1)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-mgmt-network==21.0.1->azureml-core~=1.53.0->azureml-sdk) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-core<2.0.0->azureml-core~=1.53.0->azureml-sdk) (4.7.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-core<2.0.0->azureml-core~=1.53.0->azureml-sdk) (1.16.0)\n",
      "Requirement already satisfied: pyarrow<=11.0.0,>=0.17.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (9.0.0)\n",
      "Collecting azureml-dataprep<4.14.0a,>=4.12.0a\n",
      "  Downloading azureml_dataprep-4.12.4-py3-none-any.whl (38.2 MB)\n",
      "     |████████████████████████████████| 38.2 MB 61 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fusepy<4.0.0,>=3.0.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (3.0.1)\n",
      "Requirement already satisfied: azure-identity>=1.7.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.12.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azureml-dataprep-rslex~=2.19.5dev0\n",
      "  Downloading azureml_dataprep_rslex-2.19.5-cp39-cp39-macosx_10_9_x86_64.whl (18.3 MB)\n",
      "     |████████████████████████████████| 18.3 MB 638 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (4.19.0)\n",
      "Requirement already satisfied: dotnetcore2<4.0.0,>=3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (3.1.23)\n",
      "Requirement already satisfied: cloudpickle<3.0.0,>=1.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied: azureml-dataprep-native<39.0.0,>=38.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (38.0.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (6.0)\n",
      "Collecting azureml-pipeline-core~=1.53.0\n",
      "  Downloading azureml_pipeline_core-1.53.0-py3-none-any.whl (313 kB)\n",
      "     |████████████████████████████████| 313 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-pipeline-steps~=1.53.0\n",
      "  Downloading azureml_pipeline_steps-1.53.0-py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting azureml-telemetry~=1.53.0\n",
      "  Downloading azureml_telemetry-1.53.0-py3-none-any.whl (30 kB)\n",
      "Collecting azureml-automl-core~=1.53.0\n",
      "  Downloading azureml_automl_core-1.53.0-py3-none-any.whl (248 kB)\n",
      "     |████████████████████████████████| 248 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: applicationinsights in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-telemetry~=1.53.0->azureml-train-automl-client~=1.53.0->azureml-sdk) (0.11.10)\n",
      "Collecting azureml-train-restclients-hyperdrive~=1.53.0\n",
      "  Downloading azureml_train_restclients_hyperdrive-1.53.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.53.0->azureml-sdk) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.53.0->azureml-sdk) (2.20)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from docker<7.0.0->azureml-core~=1.53.0->azureml-sdk) (1.5.1)\n",
      "Requirement already satisfied: distro>=1.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from dotnetcore2<4.0.0,>=3.0.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.8.0)\n",
      "Requirement already satisfied: tabulate in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from knack~=0.10.0->azureml-core~=1.53.0->azureml-sdk) (0.9.0)\n",
      "Requirement already satisfied: pygments in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from knack~=0.10.0->azureml-core~=1.53.0->azureml-sdk) (2.10.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msal-extensions<=1.0.0,>=0.3.0->azureml-core~=1.53.0->azureml-sdk) (2.7.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (0.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (2022.12.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (1.3.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.53.0->azureml-sdk) (0.4.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from packaging<=23.0,>=20.0->azureml-core~=1.53.0->azureml-sdk) (3.0.4)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.53.0->azureml-sdk) (4.0.1)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.53.0->azureml-sdk) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from pyarrow<=11.0.0,>=0.17.0->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.22.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (3.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied: jeepney>=0.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from SecretStorage<4.0.0->azureml-core~=1.53.0->azureml-sdk) (0.8.0)\n",
      "Requirement already satisfied: backports.weakref in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from backports.tempfile->azureml-core~=1.53.0->azureml-sdk) (1.0.post1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (0.30.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (0.9.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (23.1.0)\n",
      "Installing collected packages: azureml-dataprep-rslex, azure-mgmt-network, azureml-dataprep, azureml-core, azureml-train-restclients-hyperdrive, azureml-telemetry, azureml-dataset-runtime, azureml-train-core, azureml-automl-core, azureml-train-automl-client, azureml-pipeline-core, azureml-pipeline-steps, azureml-pipeline, azureml-sdk\n",
      "  Attempting uninstall: azureml-dataprep-rslex\n",
      "    Found existing installation: azureml-dataprep-rslex 2.15.2\n",
      "    Uninstalling azureml-dataprep-rslex-2.15.2:\n",
      "      Successfully uninstalled azureml-dataprep-rslex-2.15.2\n",
      "  Attempting uninstall: azureml-dataprep\n",
      "    Found existing installation: azureml-dataprep 4.8.6\n",
      "    Uninstalling azureml-dataprep-4.8.6:\n",
      "      Successfully uninstalled azureml-dataprep-4.8.6\n",
      "  Attempting uninstall: azureml-core\n",
      "    Found existing installation: azureml-core 1.48.0\n",
      "    Uninstalling azureml-core-1.48.0:\n",
      "      Successfully uninstalled azureml-core-1.48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: azureml-train-restclients-hyperdrive\n",
      "    Found existing installation: azureml-train-restclients-hyperdrive 1.48.0\n",
      "    Uninstalling azureml-train-restclients-hyperdrive-1.48.0:\n",
      "      Successfully uninstalled azureml-train-restclients-hyperdrive-1.48.0\n",
      "  Attempting uninstall: azureml-telemetry\n",
      "    Found existing installation: azureml-telemetry 1.48.0\n",
      "    Uninstalling azureml-telemetry-1.48.0:\n",
      "      Successfully uninstalled azureml-telemetry-1.48.0\n",
      "  Attempting uninstall: azureml-dataset-runtime\n",
      "    Found existing installation: azureml-dataset-runtime 1.48.0\n",
      "    Uninstalling azureml-dataset-runtime-1.48.0:\n",
      "      Successfully uninstalled azureml-dataset-runtime-1.48.0\n",
      "  Attempting uninstall: azureml-train-core\n",
      "    Found existing installation: azureml-train-core 1.48.0\n",
      "    Uninstalling azureml-train-core-1.48.0:\n",
      "      Successfully uninstalled azureml-train-core-1.48.0\n",
      "  Attempting uninstall: azureml-automl-core\n",
      "    Found existing installation: azureml-automl-core 1.48.0\n",
      "    Uninstalling azureml-automl-core-1.48.0:\n",
      "      Successfully uninstalled azureml-automl-core-1.48.0\n",
      "  Attempting uninstall: azureml-train-automl-client\n",
      "    Found existing installation: azureml-train-automl-client 1.48.0\n",
      "    Uninstalling azureml-train-automl-client-1.48.0:\n",
      "      Successfully uninstalled azureml-train-automl-client-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline-core\n",
      "    Found existing installation: azureml-pipeline-core 1.48.0\n",
      "    Uninstalling azureml-pipeline-core-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-core-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline-steps\n",
      "    Found existing installation: azureml-pipeline-steps 1.48.0\n",
      "    Uninstalling azureml-pipeline-steps-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-steps-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline\n",
      "    Found existing installation: azureml-pipeline 1.48.0\n",
      "    Uninstalling azureml-pipeline-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-1.48.0\n",
      "  Attempting uninstall: azureml-sdk\n",
      "    Found existing installation: azureml-sdk 1.48.0\n",
      "    Uninstalling azureml-sdk-1.48.0:\n",
      "      Successfully uninstalled azureml-sdk-1.48.0\n",
      "Successfully installed azure-mgmt-network-21.0.1 azureml-automl-core-1.53.0 azureml-core-1.53.0 azureml-dataprep-4.12.4 azureml-dataprep-rslex-2.19.5 azureml-dataset-runtime-1.53.0 azureml-pipeline-1.53.0 azureml-pipeline-core-1.53.0 azureml-pipeline-steps-1.53.0 azureml-sdk-1.53.0 azureml-telemetry-1.53.0 azureml-train-automl-client-1.53.0 azureml-train-core-1.53.0 azureml-train-restclients-hyperdrive-1.53.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azureml-sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e934f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/aml_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/aml_config.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "import json\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "#def create_ml_client(subscription_id: str, resource_group: str, workspace_name: str, tenant_id: str = None):\n",
    "def create_ml_client():\n",
    "    \"\"\"\n",
    "    Create an Azure Machine Learning workspace client.\n",
    "\n",
    "    This function attempts to create an Azure Machine Learning workspace client using the provided parameters. If it fails\n",
    "    to create a client, it generates a new configuration file with the provided parameters and tries again.\n",
    "\n",
    "    Parameters:\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group (str): Azure resource group name.\n",
    "        workspace_name (str): Azure Machine Learning workspace name.\n",
    "        tenant_id (str, optional): Azure Active Directory tenant ID. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        azureml.core.Workspace: An Azure Machine Learning workspace client.\n",
    "    \"\"\"\n",
    "    # Create an Azure CLI credential\n",
    "    credentials = AzureCliCredential(tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')\n",
    "    \n",
    "    try:\n",
    "        # Try to create the Azure Machine Learning workspace client using provided parameters\n",
    "        ml_client = Workspace.from_config(auth=credentials)\n",
    "    except Exception as ex:\n",
    "        print(\"An error occurred while creating the AML client:\", str(ex))\n",
    "        print(\"Creating a new configuration file...\")\n",
    "\n",
    "        # Define the workspace configuration based on the provided parameters\n",
    "        client_config = {\n",
    "            \"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group\": \"practice_resource\",\n",
    "            \"workspace_name\": \"practice_workspace\",\n",
    "        }\n",
    "\n",
    "        # Write the configuration to a JSON file\n",
    "        config_path = \"../config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            json.dump(client_config, fo)\n",
    "        \n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        ml_client = MLClient.from_config(credential=credentials, path=config_path)\n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        #ml_client = Workspace.from_config(path=config_path)\n",
    "    return ml_client\n",
    "   \n",
    "\n",
    "\n",
    "def get_compute(ml_client, compute_name:str, vm_size:str, min_instance:int, max_instances:int):\n",
    "    ml_client = create_ml_client()\n",
    "    # specify aml compute name.\n",
    "    cpu_compute_target = compute_name\n",
    "    \n",
    "    try:\n",
    "        cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "        print(f'Using existing compute target: {cpu_compute_target}')\n",
    "    except KeyError:\n",
    "        print(f\"Creating a new cpu compute target: {cpu_compute_target}...\")\n",
    "        cpu_cluster = AmlCompute(\n",
    "            name = cpu_compute_target,\n",
    "            size=vm_size,\n",
    "            min_nodes=min_instance,\n",
    "            max_nodes=max_instances\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).result()\n",
    "        \n",
    "    return cpu_compute_target, cpu_cluster   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a92b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/data_ingestion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_ingestion.py\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace, Experiment, Run, Dataset, Datastore\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    try:\n",
    "        # pipeline run\n",
    "        ws = run.experiment.workspace\n",
    "    except:\n",
    "        ws = Workspace.from_config()\n",
    "    # Make sure that 'ds' is a valid datastore name\n",
    "    #if ds is None:\n",
    "        #raise ValueError(\"The 'ds' parameter cannot be None.\")\n",
    "        \n",
    "    \n",
    "    #def_ds = Datastore.get(ws, ds)\n",
    "    def_ds = Datastore.get(ws, 'workspaceblobstore')\n",
    "    #load_data('workspaceblobstore', 'ai4i2020.csv')\n",
    "    raw_df = Dataset.Tabular.from_delimited_files([(def_ds,'ai4i2020.csv')]).to_pandas_dataframe()\n",
    "    log.info('Data Loaded')\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def set_cwd_path(path: str):\n",
    "    os.chdir(path)\n",
    "    log.info(f'Current Directory set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1fad0",
   "metadata": {},
   "source": [
    "### Create custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c43815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f248a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: general_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8.*\n",
    "  - pip=23.2.*\n",
    "  - pip:\n",
    "    - numpy==1.22.*\n",
    "    - mlflow==2.4.1\n",
    "    - azureml-mlflow==1.53.0\n",
    "    - azureml-core==1.53.*\n",
    "    - azureml-defaults==1.53.*\n",
    "    - scikit-learn==1.3.*\n",
    "    - azure-ai-ml==1.9.0\n",
    "    - requests==2.31.*\n",
    "    - azure-identity==1.14.0\n",
    "    - scipy==1.7.1\n",
    "    - pandas==1.4.4\n",
    "    - shap==0.42.1\n",
    "    - joblib==1.3.2\n",
    "    - seaborn==0.11.2\n",
    "    - matplotlib==3.4.*\n",
    "    - shapely==2.0.*\n",
    "    - scikit-optimize==0.9.*\n",
    "    - mldesigner==0.1.0b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3459c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d786bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencies/environment_register.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/environment_register.py\n",
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "custom_env_name = \"general_environment\"\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "env_docker_conda = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for classification and regression tasks\",\n",
    "    conda_file=\"conda.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.4.0\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06113a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-storage-file-datalake azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee829ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_prep.yaml\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  raw_data: \n",
    "    type: uri_folder \n",
    "outputs:\n",
    "  prep_data:\n",
    "    type: uri_folder\n",
    "code: ./prep_src\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python prep.py \n",
    "  --raw_data ${{inputs.raw_data}} \n",
    "  --prep_data ${{outputs.prep_data}}  # Reference 'output_data' as an input\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df325dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mldesigner in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (0.1.0b15)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (6.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (4.7.1)\n",
      "Requirement already satisfied: pydash>=5.1.2 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (5.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mldesigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4a51e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-file-datalake in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (12.12.0)\n",
      "Requirement already satisfied: azure-identity in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-storage-file-datalake) (1.29.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-storage-file-datalake) (0.6.1)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.17.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-storage-file-datalake) (12.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-storage-file-datalake) (4.7.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-identity) (3.4.8)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-identity) (1.0.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-identity) (1.21.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-identity) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.26.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cryptography>=2.5->azure-identity) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.20)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msal<2.0.0,>=1.12.0->azure-identity) (2.1.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (2.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-file-datalake) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage-file-datalake azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9bf66edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.filedatalake import (\n",
    "    DataLakeServiceClient,\n",
    "    DataLakeDirectoryClient,\n",
    "    FileSystemClient\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9930377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_client_token_credential(account_name) -> DataLakeServiceClient:\n",
    "    account_url = f\"https://{account_name}.dfs.core.windows.net\"\n",
    "    token_credential = DefaultAzureCredential()\n",
    "\n",
    "    service_client = DataLakeServiceClient(account_url, credential=token_credential)\n",
    "\n",
    "    return service_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "29e4c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_client = get_service_client_token_credential('practiceworksp0600133129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "916f6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_system(service_client: DataLakeServiceClient, file_system_name: str) -> FileSystemClient:\n",
    "    file_system_client = service_client.create_file_system(file_system=file_system_name)\n",
    "\n",
    "    return file_system_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c4bf9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_system_client = create_file_system(service_client, 'machinedata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8db2cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(file_system_client: FileSystemClient, directory_name: str) -> DataLakeDirectoryClient:\n",
    "    directory_client = file_system_client.create_directory(directory_name)\n",
    "\n",
    "    return directory_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b216dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_client = create_directory(file_system_client, 'raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "11df2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_directory(directory_client: DataLakeDirectoryClient, local_path: str, file_name: str):\n",
    "    file_client = directory_client.get_file_client(file_name)\n",
    "\n",
    "    with open(file=os.path.join(local_path, file_name), mode=\"rb\") as data:\n",
    "        file_client.upload_data(data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "30ead079",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_to_directory(directory_client, '/Users/ejenamvictor/Desktop/project_new/data/', 'ai4i2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c4359e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedWriter name='ai4i2020.csv'>\n",
      "<azure.storage.blob._download.StorageStreamDownloader object at 0x7ff46e3b9fa0>\n",
      "It takes 0.7104520797729492 seconds to download raw_data\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "STORAGEACCOUNTURL= 'https://practiceworksp0600133129.blob.core.windows.net/'\n",
    "STORAGEACCOUNTKEY= 'n6nfy8OrxQyqJ/qesdDFgQqzqY8BD/zjxjKDHzdAoMmw+yCq2nZGz5DJvg5FkAJedaxHKE3ORQ3B+ASttIXhFA=='\n",
    "LOCALFILENAME= 'ai4i2020.csv'\n",
    "CONTAINERNAME= 'machinedata'\n",
    "BLOBNAME= 'raw_data'\n",
    "\n",
    "#download from blob\n",
    "t1=time.time()\n",
    "blob_service_client_instance = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\n",
    "blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n",
    "with open(LOCALFILENAME, \"wb\") as my_blob:\n",
    "    blob_data = blob_client_instance.download_blob()\n",
    "    blob_data.readinto(my_blob)\n",
    "t2=time.time()\n",
    "print(my_blob)\n",
    "print(blob_data)\n",
    "print((\"It takes %s seconds to download \"+BLOBNAME) % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "72bb3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "STORAGEACCOUNTURL = 'https://practiceworksp0600133129.blob.core.windows.net/'\n",
    "STORAGEACCOUNTKEY = 'U79kyC7XigWJ6nROgYY5xjfaNDj82allYrfay0oPHRmomPdTkBPviebxqGceM5kdp58X1xcEZNiV+AStwp3wiQ=='\n",
    "LOCALFILENAME = 'ai4i2020'\n",
    "CONTAINERNAME = 'machinedata'\n",
    "BLOBNAME = 'raw_data'\n",
    "https://practiceworksp0600133129.blob.core.windows.net/machinedata/raw_data/ai4i2020.csv\n",
    "# Download from blob\n",
    "t1 = time.time()\n",
    "blob_service_client_instance = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\n",
    "blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n",
    "with open(LOCALFILENAME, \"wb\") as my_blob:\n",
    "    blob_data = blob_client_instance.download_blob()\n",
    "    blob_data.readinto(my_blob)\n",
    "    print(blob_data.readall())\n",
    "t2 = time.time()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "#print((\"It takes %s seconds to download \" + BLOBNAME) % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c876e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCALFILE is the file path\n",
    "#dataframe_blobdata = pd.read_csv(blob_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "4a23d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AzureDataLakeGen2Datastore({'type': <DatastoreType.AZURE_DATA_LAKE_GEN2: 'AzureDataLakeGen2'>, 'name': 'machinedatas', 'description': 'this data store contains data for a machine failures', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourceGroups/practice_resource/providers/Microsoft.MachineLearningServices/workspaces/practice_workspace/datastores/machinedatas', 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ff48b556dc0>, 'credentials': <azure.ai.ml.entities._credentials.NoneCredentialConfiguration object at 0x7ff48b556f10>, 'account_name': 'practiceworksp0600133129', 'filesystem': 'machine', 'endpoint': 'core.windows.net', 'protocol': 'https'})"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AzureDataLakeGen2Datastore\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '/Users/ejenamvictor/Desktop/project_new/modules')\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "#sys.path.append(src_dir)\n",
    "\n",
    "from aml_config_functions import *\n",
    "#from pipeline import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "#ml_client = MLClient.from_config()\n",
    "\n",
    "store = AzureDataLakeGen2Datastore(\n",
    "    name=\"machinedatas\",\n",
    "    description=\"this data store contains data for a machine failures\",\n",
    "    account_name=\"practiceworksp0600133129\",\n",
    "    filesystem=\"machine\"\n",
    ")\n",
    "\n",
    "ml_client.create_or_update(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7756d73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Datastore: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# Define your Azure Machine Learning workspace\n",
    "workspace = Workspace.from_config(path=\"config.json\")\n",
    "\n",
    "# Retrieve the data store you want to set as default\n",
    "datastore_name = \"workspaceblobstore\"\n",
    "datastore = Datastore.get(workspace, datastore_name)\n",
    "\n",
    "# Set the data store as the default\n",
    "datastore.set_as_default()\n",
    "\n",
    "# Verify the default data store\n",
    "default_datastore = Datastore.get_default(workspace)\n",
    "print(f\"Default Datastore: {default_datastore.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Specify the name of the Datastore where you want to upload data\n",
    "datastore_name = \"machinedata\"\n",
    "\n",
    "# Get a reference to the Datastore\n",
    "datastore = Datastore.get(ml_client, datastore_name)\n",
    "\n",
    "# Specify the local path to the file or folder you want to upload\n",
    "local_path = \"path/to/local/file_or_folder\"\n",
    "\n",
    "# Specify the target path in the Datastore where you want to upload the data\n",
    "target_path = \"target/path/in/datastore\"\n",
    "\n",
    "# Upload the data to the Datastore\n",
    "datastore.upload(src_dir=local_path, target_path=target_path, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ce634724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0    1     M14860    M                298.1                    308.6   \n",
       "1    2     L47181    L                298.2                    308.7   \n",
       "2    3     L47182    L                298.1                    308.5   \n",
       "3    4     L47183    L                298.2                    308.6   \n",
       "4    5     L47184    L                298.2                    308.7   \n",
       "\n",
       "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Machine failure  TWF  \\\n",
       "0                    1551         42.8                0                0    0   \n",
       "1                    1408         46.3                3                0    0   \n",
       "2                    1498         49.4                5                0    0   \n",
       "3                    1433         39.5                7                0    0   \n",
       "4                    1408         40.0                9                0    0   \n",
       "\n",
       "   HDF  PWF  OSF  RNF  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "2    0    0    0    0  \n",
       "3    0    0    0    0  \n",
       "4    0    0    0    0  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "df = pd.read_csv('/Users/ejenamvictor/Desktop/project_new/data/ai4i2020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "72b8c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "df = pd.read_csv('/Users/ejenamvictor/Desktop/project_new/data/ai4i2020.csv')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Machine failure'], random_state=101)\n",
    "\n",
    "# Define local directories for train and test data\n",
    "train_data_dir = 'train_data'\n",
    "test_data_dir = 'test_data'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(train_data_dir, exist_ok=True)\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save the train and test data to the respective directories as CSV files\n",
    "train_df.to_csv(os.path.join(train_data_dir, 'train_df.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(test_data_dir, 'test_df.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9baf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "\n",
    "machine_ds = Input(\n",
    "    path='/Users/ejenamvictor/Desktop/project_CAS/ai4i2020.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "e88dc1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "parent_dir = \"\"\n",
    "\n",
    "# 1. Load components\n",
    "prepare_data = load_component(source=parent_dir + \"./data_prep.yaml\")\n",
    "train_model = load_component(source=parent_dir + \"./train.yaml\")\n",
    "test_model = load_component(source=parent_dir + \"./test.yaml\")\n",
    "#modules = load_component(source=parent_dir + \"./modules.yaml\")\n",
    "\n",
    "# 2. Construct pipeline\n",
    "@pipeline()\n",
    "def machine_failure_classification(pipeline_job_input,\n",
    "                                  train_input,\n",
    "                                  test_input,\n",
    "                                  inference_output, \n",
    "                                  ):\n",
    "    \"\"\"Machine failure classification.\"\"\"\n",
    "    prepare_sample_data = prepare_data(raw_data=pipeline_job_input)\n",
    "    train_with_sample_data = train_model(train_data=train_input)\n",
    "    test_model_performance = test_model(test_data=test_input,\n",
    "                                       model_input=train_with_sample_data.outputs.model_output,)\n",
    "    #modules_pipeline = modules()\n",
    "    return {\n",
    "        \"pipeline_job_prepped_data\": prepare_sample_data.outputs.prep_data,\n",
    "        \"pipeline_job_model\": train_with_sample_data.outputs.model_output,\n",
    "        \"pipeline_inference_data\": test_model_performance.outputs.inference_df,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_jobs = machine_failure_classification(\n",
    "    pipeline_job_input = Input(type=\"uri_folder\", path=parent_dir + \"./data/\"),\n",
    "    train_input = Input(type=\"uri_folder\", path=parent_dir + \"./train_data/\"),\n",
    "    test_input = Input(type=\"uri_folder\", path=parent_dir + \"./test_data/\"),\n",
    "    inference_output = 'inference_data'\n",
    ")\n",
    "# demo how to change pipeline output settings\n",
    "#pipeline_jobs.outputs.pipeline_job_prepped_data.mode = \"rw_mount\"\n",
    "#pipeline_jobs.outputs.pipeline_job_train_data.mode = \"rw_mount\"\n",
    "#pipeline_jobs.outputs.pipeline_job_test_data.mode = \"rw_mount\"\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_jobs.settings.default_compute = \"cpu-cluster\"\n",
    "# set pipeline level datastore\n",
    "pipeline_jobs.settings.default_datastore = \"workspaceblobstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "eb349938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_new/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n",
      "Uploading project_new (1.59 MBs):   8%| | 132052/1594125 [00:00<00:05, 268502.48WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "Uploading project_new (1.59 MBs):  21%|▏| 341904/1594125 [00:01<00:02, 517635.98WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "Uploading project_new (1.59 MBs):  35%|▎| 558012/1594125 [00:01<00:01, 610948.11WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "Uploading project_new (1.59 MBs):  74%|▋| 1182950/1594125 [00:01<00:00, 1031698.WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: practiceworksp0600133129.blob.core.windows.net\n",
      "Uploading project_new (1.59 MBs): 100%|█| 1594125/1594125 [00:01<00:00, 813684.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>machine_failure_pipeline_recent</td><td>orange_loquat_c9sth1zwbb</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/orange_loquat_c9sth1zwbb?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace&amp;tid=6aa8da55-4c6f-496e-8fc1-de0f7819b03b\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7ff47316ed90>, 'train_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7ff47316ee20>, 'test_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7ff47316ed00>, 'inference_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7ff47316ea60>}, 'outputs': {'pipeline_job_prepped_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7ff47316e520>, 'pipeline_job_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7ff47316e850>, 'pipeline_inference_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7ff47316e9a0>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': 'Machine failure classification.', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ff47316e2b0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'machine_failure_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}, 'train_input': {}, 'test_input': {}, 'inference_output': {}}, 'outputs': {'pipeline_job_prepped_data': {}, 'pipeline_job_model': {}, 'pipeline_inference_data': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'prepare_sample_data': Command({'parameters': {}, 'init': False, 'name': 'prepare_sample_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ff47316ea90>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (INFO)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'raw_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'prep_data': '${{parent.outputs.pipeline_job_prepped_data}}'}, 'inputs': {'raw_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7ff47316e2e0>}, 'outputs': {'prep_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7ff47316e550>}, 'component': 'azureml_anonymous:cdb4a3f4-1298-4c13-95e4-d294141dfae5', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'f0c5d04e-70db-4d50-882a-c644e2879f7e', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_with_sample_data': Command({'parameters': {}, 'init': False, 'name': 'train_with_sample_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ff47316e280>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (INFO)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'train_data': '${{parent.inputs.train_input}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_model}}'}, 'inputs': {'train_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7ff47316ef10>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7ff47316e3a0>}, 'component': 'azureml_anonymous:81d315c1-778b-4634-8116-90a3c3b93af7', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '6abf1030-228d-42aa-92e5-e637dfa2216b', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'test_model_performance': Command({'parameters': {}, 'init': False, 'name': 'test_model_performance', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ff47316e4f0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (INFO)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'test_data': '${{parent.inputs.test_input}}', 'model_input': '${{parent.jobs.train_with_sample_data.outputs.model_output}}'}, 'job_outputs': {'inference_df': '${{parent.outputs.pipeline_inference_data}}'}, 'inputs': {'test_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7ff47316eac0>, 'model_input': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7ff47316e7f0>}, 'outputs': {'inference_df': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7ff47316e6a0>}, 'component': 'azureml_anonymous:38c9ab7a-d353-4d31-9183-c621ebde1f18', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'd3f366af-982b-4e3b-9613-7829defd78ea', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 3}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 3}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'orange_loquat_c9sth1zwbb', 'description': 'Machine failure classification.', 'tags': {}, 'properties': {'mlflow.source.git.branch': 'master', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"inference_output\":\"inference_data\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourceGroups/practice_resource/providers/Microsoft.MachineLearningServices/workspaces/practice_workspace/jobs/orange_loquat_c9sth1zwbb', 'Resource__source_path': None, 'base_path': '/Users/ejenamvictor/Desktop/project_new', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7ff47316e040>, 'serialize': <msrest.serialization.Serializer object at 0x7ff47316e9d0>, 'display_name': 'machine_failure_classification', 'experiment_name': 'machine_failure_pipeline_recent', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourceGroups/practice_resource/providers/Microsoft.MachineLearningServices/workspaces/practice_workspace?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/orange_loquat_c9sth1zwbb?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace&tid=6aa8da55-4c6f-496e-8fc1-de0f7819b03b', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '/Users/ejenamvictor/Desktop/project_new/modules')\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "#sys.path.append(src_dir)\n",
    "\n",
    "from aml_config_functions import *\n",
    "#from pipeline import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "# submit job to workspace\n",
    "pipeline_jobs = ml_client.jobs.create_or_update(\n",
    "    pipeline_jobs, experiment_name=\"machine_failure_pipeline_recent\"\n",
    ")\n",
    "pipeline_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "e44b6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0, '/Users/ejenamvictor/Desktop/project_new/modules')\n",
    "#sys.path.append(function_dir)\n",
    "sys.path.insert(0, '/Users/ejenamvictor/Desktop/project_new/src/modules')\n",
    "#sys.path.insert(0, \"./src/modules\")\n",
    "# Now you can import modules from function_dir\n",
    "#from modules.split_data import *\n",
    "#from tune_train_test import *\n",
    "#from aml_config_functions import *\n",
    "#from modules import split_data as sd\n",
    "#from modules import tune_train_test as tt\n",
    "#from modules import aml_config_functions as acf\n",
    "#import split_data as sd\n",
    "import tune_train_test as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_models(model_name, predictions, true_labels, target_names, plot_classification_report=False, artifact_save_dir: str = 'artifacts'):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model and generate a classification report.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model for plot naming.\n",
    "        predictions (array-like): Model predictions (predicted class labels).\n",
    "        true_labels (array-like): True class labels.\n",
    "        target_names (list): e.g ['Class 0', 'Class 1']\n",
    "        plot_classification_report (bool): Whether to plot the classification report (default: False).\n",
    "        artifact_save_dir (str): Directory where artifacts including plots will be saved.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed metrics.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    evaluation_results['accuracy'] = accuracy\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    evaluation_results['precision'] = precision\n",
    "\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    evaluation_results['recall'] = recall\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    evaluation_results['f1'] = f1\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels, predictions)\n",
    "        evaluation_results['roc_auc'] = roc_auc\n",
    "    except ValueError:\n",
    "        # roc_auc_score may not work for multiclass classification\n",
    "        evaluation_results['roc_auc'] = None\n",
    "\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    evaluation_results['confusion_matrix'] = confusion\n",
    "\n",
    "    if plot_classification_report:\n",
    "        # Generate the classification report\n",
    "        report = classification_report(\n",
    "            true_labels, predictions, target_names=target_names, output_dict=True\n",
    "        )\n",
    "\n",
    "        # Convert the classification report to a DataFrame for easy plotting\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "        # Plot the classification report as a heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(report_df.iloc[:-3, :-1], annot=True, cmap='Blues', fmt=\".2f\", cbar=False)\n",
    "        plt.title('Classification Report')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Metrics')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        model_prefix = os.path.splitext(model_name)[0]\n",
    "        plot_name = os.path.join(artifact_save_dir, f'{model_prefix}_Classification_Report.png')\n",
    "        plt.savefig(plot_name)\n",
    "        log.info(f'{plot_name} saved.')\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "        # Add classification report metrics to the evaluation results\n",
    "        for metric, values in report.items():\n",
    "            if isinstance(values, dict):\n",
    "                for class_name, value in values.items():\n",
    "                    metric_name = f\"{metric}_{class_name}\"\n",
    "                    evaluation_results[metric_name] = value\n",
    "    log.info(evaluation_results)\n",
    "    return evaluation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8742b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_dir = \"./test_src\"\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d42cec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.yaml\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: test\n",
    "display_name: test_model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  test_data: \n",
    "    type: uri_folder\n",
    "  model_input:\n",
    "    type: mlflow_model\n",
    "outputs:\n",
    "  inference_df:\n",
    "    type: uri_folder\n",
    "#code: ./test_src\n",
    "code: ./\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python \n",
    "  test_src/test.py\n",
    "  --test_data ${{inputs.test_data}}\n",
    "  --model_input ${{inputs.model_input}}\n",
    "  --inference_df ${{outputs.inference_df}}  # Reference 'output_data' as an input\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "2f9b6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.yaml\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train\n",
    "display_name: train_model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "#code: ./train_src\n",
    "code: ./\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python \n",
    "  train_src/train.py\n",
    "  --train_data ${{inputs.train_data}}\n",
    "  --model_output ${{outputs.model_output}} # Reference 'output_data' as an input\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "90ac67a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules.yaml\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: modules\n",
    "display_name: functions\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "code: ./modules\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python \n",
    "  aml_config_functions.py\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36624a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b573de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./test_src/test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {test_dir}/test.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing_extensions import Concatenate\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sys\n",
    "import mlflow\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '.', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "#sys.path.append(function_dir)\n",
    "\n",
    "# Now you can import modules from function_dir\n",
    "import split_data as sd\n",
    "import tune_train_test as tt\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "#from modules import data_prep_functions\n",
    "\n",
    "#from aml_config_functions import *\n",
    "#sys.path.insert(1, '/Users/ejenamvictor/Desktop/project_new/modules')\n",
    "#from data_prep_functions import *\n",
    "#from split_data import *\n",
    "#from tune_train_test import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--test_data\", type=str, help=\"Path to raw data\")\n",
    "parser.add_argument(\"--model_input\", type=str, help=\"trained model\")\n",
    "parser.add_argument(\"--inference_df\", type=str, help=\"Path to store inference data\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "test_df = pd.read_csv(select_first_file(args.test_data), index=False)\n",
    "\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n",
    "\n",
    "# Assuming 'test_data' contains both features and the target column\n",
    "X_test = test_df.drop(columns=['Machine failure'])\n",
    "y_test = test_df['Machine failure']\n",
    "\n",
    "# load mlflow model\n",
    "model = mlflow.sklearn.load_model(args.model_input)\n",
    "\n",
    "inference_df, inference_col_name, predictions = predict_model(model, X_test, inference_col_name)\n",
    "\n",
    "inference_dir = Path(args.inference_df)\n",
    "inference_dir.mkdir(parents=True, exist_ok=True)\n",
    "inference_df.to_csv(inference_dir / \"inference_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cbe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586109d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00259b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = \"./train_src\"\n",
    "os.makedirs(train_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3904c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.yaml\n",
    "# <component>\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_test\n",
    "display_name: training_data\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "code: \n",
    "    - ./train_src\n",
    "    - ./modules\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python \n",
    "  train.py\n",
    "  --input_data ${{inputs.input_data}} \n",
    "  --model_output ${{outputs.model_input}}  # Reference 'output_data' as an input\n",
    "  --train_output ${{inputs.train_input}}\n",
    "  --test_output ${{inputs.test_output}}\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a2efba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./train_src/train.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile {train_dir}/train.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing_extensions import Concatenate\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sys\n",
    "import mlflow\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '.', 'test')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "#sys.path.append(function_dir)\n",
    "\n",
    "# Now you can import modules from function_dir\n",
    "import split_data as sd\n",
    "import tune_train_test as tt\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "#from modules import data_prep_functions\n",
    "\n",
    "#from aml_config_functions import *\n",
    "#sys.path.insert(1, '/Users/ejenamvictor/Desktop/project_new/modules')\n",
    "#from data_prep_functions import *\n",
    "#from split_data import *\n",
    "#from tune_train_test import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"Path to raw data\")\n",
    "parser.add_argument(\"--HG_model_output\", type=str, help=\"Path of prepped data\")\n",
    "parser.add_argument(\"--X_train_output\", type=str, help=\"Path of prepped data\")\n",
    "parser.add_argument(\"--X_test_output\", type=str, help=\"Path of prepped data\")\n",
    "parser.add_argument(\"--y_train_output\", type=str, help=\"Path of prepped data\")\n",
    "parser.add_argument(\"--X_test_output\", type=str, help=\"Path of prepped data\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_df = pd.read_csv(select_first_file(args.input_data))\n",
    "\n",
    "X_train, X_test, y_train, y_test = sd.custom_train_test_split(train_df, target_column='Machine failure', test_size=0.2, random_state=101, time_series=False)\n",
    "\n",
    "X_train_data = X_train.to_csv((Path(args.X_train_output) / \"X_train.csv\"), index=False)\n",
    "X_test_data = X_test.to_csv((Path(args.X_test_output) / \"X_test.csv\"), index=False)\n",
    "y_train_data = y_train.to_csv((Path(args.y_train_output) / \"y_train.csv\"), index=False)\n",
    "y_train_data = y_train.to_csv((Path(args.y_train_output) / \"y_train.csv\"), index=False)\n",
    "\n",
    "best_params, hyperparameters_filename= tt.hyperparameter_tuning(X_train, y_train, 'test', bayesian_search=True, n_iter=30, random_seed=42)\n",
    "trained_model = tt.train_model(X_train, y_train, 'HGBR.pkl', hyperparam_filename=hyperparameters_filename)\n",
    "\n",
    "\n",
    "mlflow.sklearn.save_model(trained_model, args.HG_model_output)\n",
    "    \n",
    "# Registering the model to the workspace\n",
    "print(\"Registering the model via MLFlow\")\n",
    "mlflow.sklearn.log_model(\n",
    "    sk_model=trained_model,\n",
    "    registered_model_name='HGBC',\n",
    "    artifact_path='HGBC',\n",
    ")\n",
    "\n",
    "# Saving the model to a file\n",
    "mlflow.sklearn.save_model(\n",
    "    sk_model=trained_model,\n",
    "    path=os.path.join(args.HG_model_output, 'trained_model'),\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e282e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6098ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f71d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f0e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a28ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eddd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28da157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "parent_dir = \"\"\n",
    "\n",
    "# 1. Load components\n",
    "prepare_data = load_component(source=parent_dir + \"./data_prep.yaml\")\n",
    "train_model = load_component(source=parent_dir + \"./train_test.yaml\")\n",
    "\n",
    "# 2. Construct pipeline\n",
    "@pipeline()\n",
    "def machine_failure_classification(pipeline_job_input):\n",
    "    \"\"\"Machine failure classification.\"\"\"\n",
    "    prepare_sample_data = prepare_data(raw_data=pipeline_job_input)\n",
    "    train_with_sample_data = train_model(\n",
    "        input_data=prepare_sample_data\n",
    "    )\n",
    "    return {\n",
    "        \"pipeline_job_prepped_data\": prepare_sample_data.outputs.prep_data,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_jobs = machine_failure_classification(\n",
    "    Input(type=\"uri_folder\", path=parent_dir + \"./data/\"),\n",
    "    X_train_output = Output(type='uri_folder', path=parent_dir + './test/') \n",
    ")\n",
    "# demo how to change pipeline output settings\n",
    "pipeline_jobs.outputs.pipeline_job_prepped_data.mode = \"rw_mount\"\n",
    "\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_jobs.settings.default_compute = \"cpu-cluster\"\n",
    "# set pipeline level datastore\n",
    "pipeline_jobs.settings.default_datastore = \"workspaceblobstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667b6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d34f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69ec8f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing {prep_src}/prep.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '{prep_src}/prep.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/y7kf04rj02xbdjnpf7k716yw0000gn/T/ipykernel_90067/2172748292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{prep_src}/prep.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'import argparse\\nfrom pathlib import Path\\nfrom uuid import uuid4\\nfrom datetime import datetime\\nimport os\\n\\nparser = argparse.ArgumentParser(\"data_preparation\")\\nparser.add_argument(\"--input_data\", type=str, help=\"Path to training data\")\\nparser.add_argument(\"--ms_threshold\", type=int, help=\"Max # of epochs for the training\")\\nparser.add_argument(\"--corr_threshold\", type=float, help=\"Learning rate\")\\nparser.add_argument(\"--plot_heatmaps\", type=str, help=\"Learning rate schedule\")\\nparser.add_argument(\"--max_unique_threshold\", type=str, help=\"Path of output model\")\\nparser.add_argument(\"--output_data\", type=str, help=\"Path of output model\")\\n\\nargs = parser.parse_args()\\n\\nprint(\"hello your data data is cooking...\")\\n\\nlines = [\\n    f\"input_data: {args.input_data}\",\\n    f\"ms_threshold: {args.ms_threshold}\",\\n    f\"corr_threshold: {args.corr_threshold}\",\\n    f\"plot_heatmaps: {args.plot_heatmaps}\",\\n    f\"max_unique_threshold: {args.max_unique_threshold}\",\\n    f\"output_data: {args.output_data}\",\\n]\\n\\nfor line in lines:\\n    print(line)\\n\\nprint(\"mounted_path files: \")\\narr = os.listdir(args.input_data)\\nprint(arr)\\n\\nfor filename in arr:\\n    print(\"reading file: %s ...\" % filename)\\n    with open(os.path.join(args.input_data, filename), \"r\") as handle:\\n        print(handle.read())\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '{prep_src}/prep.py'"
     ]
    }
   ],
   "source": [
    "#%%writefile {prep_src}/prep.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"data_preparation\")\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"Path to training data\")\n",
    "parser.add_argument(\"--ms_threshold\", type=int, help=\"Max # of epochs for the training\")\n",
    "parser.add_argument(\"--corr_threshold\", type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--plot_heatmaps\", type=str, help=\"Learning rate schedule\")\n",
    "parser.add_argument(\"--max_unique_threshold\", type=str, help=\"Path of output model\")\n",
    "parser.add_argument(\"--output_data\", type=str, help=\"Path of output model\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"hello your data data is cooking...\")\n",
    "\n",
    "lines = [\n",
    "    f\"input_data: {args.input_data}\",\n",
    "    f\"ms_threshold: {args.ms_threshold}\",\n",
    "    f\"corr_threshold: {args.corr_threshold}\",\n",
    "    f\"plot_heatmaps: {args.plot_heatmaps}\",\n",
    "    f\"max_unique_threshold: {args.max_unique_threshold}\",\n",
    "    f\"output_data: {args.output_data}\",\n",
    "]\n",
    "\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "print(\"mounted_path files: \")\n",
    "arr = os.listdir(args.input_data)\n",
    "print(arr)\n",
    "\n",
    "for filename in arr:\n",
    "    print(\"reading file: %s ...\" % filename)\n",
    "    with open(os.path.join(args.input_data, filename), \"r\") as handle:\n",
    "        print(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b45d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319484a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ff276ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_component_registration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_component_registration.py\n",
    "\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "#components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(current_directory, data_prep_yaml_file))\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "\n",
    "# Loading the component from the yaml file\n",
    "loaded_component_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(loaded_component_prep)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "136ca13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/pipeline.py\n",
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "#aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from aml_config import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "\n",
    "cpu_compute_target, cpu_cluster = get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n",
    "\n",
    "parent_directory = '../modules/'  # Adjust this to your components directory\n",
    "\n",
    "data_prep = load_component(source=parent_directory + 'data_prep.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target\n",
    "    if (cpu_cluster)\n",
    "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"first pipeline\",\n",
    ")\n",
    "def classification_pipeline(\n",
    "    input_data,\n",
    "    ms_threshold,\n",
    "    corr_threshold,\n",
    "    plot_heatmaps,\n",
    "    max_unique_threshold,\n",
    "    output_data,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep(\n",
    "        input_data = input_data,\n",
    "        ms_threshold = ms_threshold,\n",
    "        corr_threshold = corr_threshold,\n",
    "        plot_heatmaps = plot_heatmaps,\n",
    "        max_unique_threshold = max_unique_threshold\n",
    "    )\n",
    "    \n",
    "    #data_prep_job.outputs.output_data = Output(type='uri_folder', path=output_data, mode='rw_mount')\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.output_data,\n",
    "    }\n",
    "\n",
    "parent_dir = \".\"\n",
    "\n",
    "pipeline = classification_pipeline(\n",
    "    input_data=Input(type=\"uri_folder\", path= parent_dir + \"/data/\"),\n",
    "    output_data=Output(type=\"uri_folder\", path=\"processed_data\"),\n",
    "    ms_threshold = 10,\n",
    "    corr_threshold = 0.8,\n",
    "    plot_heatmaps = True,\n",
    "    max_unique_threshold = 0.9,\n",
    ")\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2348a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff259c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e68b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5a7e528",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/y7kf04rj02xbdjnpf7k716yw0000gn/T/ipykernel_90067/302055729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"load the data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     environment=dict(\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mconda_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"conda.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     ),\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#%%writefile {modules_dir}/data_prep_component.yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from mldesigner import command_component, Input, Output\n",
    "\n",
    "\n",
    "@command_component(\n",
    "    name=\"prep_data\",\n",
    "    version=\"1\",\n",
    "    display_name=\"Prep Data\",\n",
    "    description=\"load the data\",\n",
    "    environment=dict(\n",
    "        conda_file=Path(__file__).parent / \"conda.yaml\",\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    ),\n",
    ")\n",
    "def prepare_data_component(\n",
    "    input_data: Input(type=\"uri_folder\"),\n",
    "    output_data: Output(type=\"uri_folder\"),\n",
    "):\n",
    "    df = drop_high_cardinality_features(os.path.join(training_data, \"processed.csv\"))\n",
    "    \n",
    "    df = drop_high_cardinality_features(df=raw_data, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "    \n",
    "    os.path.join(training_data, \"mnist_train.csv\")\n",
    "    \n",
    "    os.makedirs(args.output_data, exist_ok=True)\n",
    "    processed_data_path = os.path.join(args.output_data, 'processed_df.csv')\n",
    "    df.to_csv(processed_data_path, engine='pyarrow')\n",
    "    \n",
    "    \n",
    "def process_data():\n",
    "    df = drop_high_cardinality_features(df=raw_data, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885849a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690480ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b94140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4346d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input_data INPUT_DATA]\n",
      "                             [--ms_threshold MS_THRESHOLD]\n",
      "                             [--corr_threshold CORR_THRESHOLD]\n",
      "                             [--plot_heatmaps PLOT_HEATMAPS]\n",
      "                             [--max_unique_threshold MAX_UNIQUE_THRESHOLD]\n",
      "                             [--output_data OUTPUT_DATA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/ejenamvictor/Library/Jupyter/runtime/kernel-bbde93c4-7b2f-4cd3-9671-2e4c4af96dfc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#%%writefile {modules_dir}/data_preparation.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "from data_prep import *\n",
    "from data_ingestion import *\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "def get_file(f):\n",
    "\n",
    "    f = Path(f)\n",
    "    if f.is_file():\n",
    "        return f\n",
    "    else:\n",
    "        files = list(f.iterdir())\n",
    "        if len(files) == 1:\n",
    "            return files[0]\n",
    "        else:\n",
    "            raise Exception(\"********This path contains more than one file*******\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\n",
    "        \"--input_data\", type=str, help=\"path containing data for scoring\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--ms_threshold', dest='ms_threshold', type=int, default=10, help='Threshold to switch between interpolation and iterative imputer'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--corr_threshold', dest='corr_threshold', type=float, default=0.8,  help='The threshold for correlation above which features will be dropped'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--plot_heatmaps', dest='plot_heatmaps', type=bool, default=True,  help='Whether to plot heatmaps before and after dropping (default is True)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max_unique_threshold', dest='max_unique_threshold', type=float, default= 0.9, help='The maximum allowed fraction of unique values in a column (default is 0.9)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_data', dest='output_data', type=str\n",
    "    )\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def clean_data(input_data, output_data):\n",
    "\n",
    "    test_file = get_file(input_data)\n",
    "    data = pd.read_csv(test_file)\n",
    "    \n",
    "    df = drop_high_cardinality_features(data, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "\n",
    "    # Output result\n",
    "    np.savetxt(output_data + \"/predict_result.csv\", df, delimiter=\",\")\n",
    "\n",
    "def main(args):\n",
    "    score(args.input_data, args.ms_threshold, args.corr_threshold, args.plot_heatmaps, args.max_unique_threshold, args.output_data)\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # call main function\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211760f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b074b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46425932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538800e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7260eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "62a240d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_preparation.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#modules_dir = os.path.join(current_dir, '..', 'modules')\n",
    "#sys.path.append(modules_dir)\n",
    "# Get the directory of the current script (assuming it's in the same directory)\n",
    "#script_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
    "\n",
    "# Add the directory containing \"data_prep.py\" to the Python path\n",
    "#sys.path.append(os.path.join(script_dir, 'modules'))\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "#aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(aml_config_dir):\n",
    "#    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "\n",
    "from data_prep import *\n",
    "from data_ingestion import *\n",
    "import argparse \n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "#def a custom argument type for a list of strings\n",
    "def list_of_strings(arg):\n",
    "    return arg.split(',')\n",
    "\n",
    "#if 'ipykernel' in sys.modules:\n",
    "    # Exclude IPython-specific arguments\n",
    "#    sys.argv = [arg for arg in sys.argv if not arg.endswith('ipykernel_launcher.py')]\n",
    "import aml_config as aml\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "def load_raw_data():\n",
    "    machine_data = ml_client.data.get(name='machine-failure', version='1')\n",
    "    data_path = machine_data.path  # Remove quotes around data_path variable\n",
    "    data = pd.read_csv(data_path)  # Use the data_path variable\n",
    "    return data  # Return the data frame\n",
    "\n",
    "def path_exists(path):\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(f\"Path {path} does not exist.\")\n",
    "   \n",
    "    \n",
    "def main():\n",
    "    # Check if running in IPython environment and exclude IPython-specific arguments\n",
    "#    if 'get_ipython' in globals():\n",
    "#        sys.argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
    "\n",
    "    #setup arg parser\n",
    "        \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_ds', dest='--input_ds', type=path_exists, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--ms_threshold', dest='ms_threshold', type=int, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--corr_threshold', dest='corr_threshold', type=float, help='The threshold for correlation above which features will be dropped')\n",
    "    parser.add_argument('--plot_heatmaps', dest='plot_heatmaps', type=bool, help='Whether to plot heatmaps before and after dropping (default is True)')\n",
    "    parser.add_argument('--max_unique_threshold', dest='max_unique_threshold', type=float, help='The maximum allowed fraction of unique values in a column (default is 0.9)')\n",
    "    parser.add_argument('--output_data', dest='output_data', type=str)\n",
    "        \n",
    "        # parse args\n",
    "    args = parser.parse_args()\n",
    "        # Start Logging\n",
    "        \n",
    "    mlflow.start_run()\n",
    "\n",
    "    #print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    #print(\"input data:\", args.data_path)\n",
    "\n",
    "    #raw_data = pd.read_csv(args.data_path, header=1, index_col=0)\n",
    "    #raw_data = pd.read_csv((Path(args.input_ds)))\n",
    "    raw_data = pd.read_csv(args.input_ds)\n",
    "        # Load in the data from synapse DataLake Storage\n",
    "        #raw_df = load_data(args.subscription_id, args.resource_group, args.workspace_name, args.data_name)\n",
    "        # Direct working directory to Artefact location\n",
    "        #set_cwd_path(args.plot_save_dir)\n",
    "        \n",
    "        #df = pd.read_csv(args.raw_data)\n",
    "        #if not os.path.exists(artifact_save_dir):\n",
    "        #os.makedirs(artifact_save_dir)\n",
    "        #if raw_data is None:\n",
    "        #    mlflow.log_info('Failed to load data.')  # Log an info message\n",
    "        #    mlflow.end_run()  # End the run\n",
    "        #    return \n",
    "            \n",
    "    raw_data = check_missing_values(raw_data)\n",
    "        # Direct working directory to Artefact location\n",
    "        #set_cwd_path(args.plot_save_dir)\n",
    "        \n",
    "            \n",
    "        #df = drop_high_cardinality_features(df=raw_data, max_unique_threshold=args.max_unique_threshold)\n",
    "        #df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "        #df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "    \n",
    "        # Reset directory back to initial working directory\n",
    "        #set_cwd_path('..')\n",
    "    \n",
    "    mlflow.log_metric('Sample Size', raw_data.shape[0])\n",
    "    os.makedirs(args.output_data, exist_ok=True)\n",
    "    processed_data_path = os.path.join(args.output_data, 'processed_df.csv')\n",
    "    df.to_csv(processed_data_path, engine='pyarrow')\n",
    "    \n",
    "        #df.to_csv(os.path.join(args.output_data, 'processed_df.csv'), engine='pyarrow')\n",
    "    \n",
    "        # End Logging\n",
    "    #except Exception as e:\n",
    "    # Log information about the exception\n",
    "    #mlflow.log_param(\"exception_type\", type(e).__name__)\n",
    "    #mlflow.log_param(\"exception_message\", str(e))\n",
    "    #mlflow.log_param(\"exception_traceback\", traceback.format_exc())\n",
    " \n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "336556cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ejenamvictor/Desktop/project_CAS/modules\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(aml_config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8881dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_component_registration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_component_registration.py\n",
    "\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "#components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(current_directory, data_prep_yaml_file))\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "\n",
    "#parent_directory = 'modules/'  # Adjust this to your components directory\n",
    "\n",
    "#loaded_component_prep = load_component(source=current_dir + '/' + 'data_prep.yaml')\n",
    "\n",
    "# Loading the component from the yaml file\n",
    "loaded_component_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(loaded_component_prep)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c2b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ejenamvictor/Desktop/project_CAS/src/data_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "data_prep_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_path = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, data_prep_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_path))\n",
    "\n",
    "print(data_prep_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c25ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d18f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_dir = \"./pipeline\"\n",
    "os.makedirs(pipeline_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "59ff0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/pipeline.py\n",
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "#aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from aml_config import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "\n",
    "cpu_compute_target, cpu_cluster = get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n",
    "\n",
    "parent_directory = '../modules/'  # Adjust this to your components directory\n",
    "\n",
    "data_prep = load_component(source=parent_directory + 'data_prep.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target\n",
    "    if (cpu_cluster)\n",
    "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"first pipeline\",\n",
    ")\n",
    "def classification_pipeline(\n",
    "    input_ds,\n",
    "    ms_threshold,\n",
    "    corr_threshold,\n",
    "    plot_heatmaps,\n",
    "    max_unique_threshold,\n",
    "    output_data,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep(\n",
    "        input_ds = input_ds,\n",
    "        ms_threshold = ms_threshold,\n",
    "        corr_threshold = corr_threshold,\n",
    "        plot_heatmaps = plot_heatmaps,\n",
    "        max_unique_threshold = max_unique_threshold\n",
    "    )\n",
    "    \n",
    "    #data_prep_job.outputs.output_data = Output(type='uri_folder', path=output_data, mode='rw_mount')\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.output_data,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f76570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d41f7c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/pipeline_job_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/pipeline_job_submission.py\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "#import pipeline as pi\n",
    "\n",
    "# Get the current working directory\n",
    "##current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "#components_relative_path = 'src'  # Adjust this to your components directory\n",
    "#data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "#aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "#component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "#data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "from aml_config import *\n",
    "from pipeline import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "#machine_data = ml_client.data.get(name='machine-failure', version='1')\n",
    "#print(f'Data asset URI:{machine_data.path}')\n",
    "\n",
    "# Define input data and parameters\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = classification_pipeline(\n",
    "    input_ds=Input(type=\"uri_file\", path='/Users/ejenamvictor/Desktop/project_CAS/ai4i2020.csv'),\n",
    "    ms_threshold = 10,\n",
    "    corr_threshold = 0.8,\n",
    "    plot_heatmaps = True,\n",
    "    max_unique_threshold = 0.9,\n",
    "    output_data = 'processed_data'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#src_dir = os.path.join(current_dir, '.', 'modules')\n",
    "#sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabfa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5880353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0bd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ee3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70325a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a1d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc26b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d61ea98d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n",
      "Found the config file in: ../config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: plucky_coat_jzptvvnsnt\n",
      "Web View: https://ml.azure.com/runs/plucky_coat_jzptvvnsnt?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-09-30 13:33:20Z] Submitting 1 runs, first five are: 8f1602fb:f40e393c-9a1a-481f-a28c-48571b0f888b\n",
      "[2023-09-30 13:37:18Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: plucky_coat_jzptvvnsnt\n",
      "Web View: https://ml.azure.com/runs/plucky_coat_jzptvvnsnt?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /data_prep_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"ukwest\",\n    \"location\": \"ukwest\",\n    \"time\": \"2023-09-30T13:37:18.037605Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/y7kf04rj02xbdjnpf7k716yw0000gn/T/ipykernel_98002/100178253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_prep_component\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mml_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/core/tracing/decorator.py\u001b[0m in \u001b[0;36mwrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mspan_impl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspan_impl_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/_telemetry/activity.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlog_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/operations/_job_operations.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPipelineChildJobError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         self._stream_logs_until_completion(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runs_operations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datastore_operations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requests_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/operations/_job_ops_helper.py\u001b[0m in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 raise JobException(\n\u001b[0m\u001b[1;32m    313\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Exception : \\n {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorTarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /data_prep_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"ukwest\",\n    \"location\": \"ukwest\",\n    \"time\": \"2023-09-30T13:37:18.037605Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "# Specify the directory containing the aml_config module\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '.', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "#aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(aml_config_dir):\n",
    "#    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "## Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b721e70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fac04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80362751",
   "metadata": {},
   "source": [
    "#%%writefile {components_dir}/data_prep.yaml\n",
    "\n",
    "#$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components_registration'  # Adjust this to your components directory\n",
    "data_prep_python_file = 'data_prep.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_prep_python_file))\n",
    "\n",
    "\n",
    "        \n",
    "# <component>\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  subscription_id:\n",
    "    type: string\n",
    "  resource_group:\n",
    "    type: string\n",
    "  workspace_name:\n",
    "    type: string\n",
    "  data_name:\n",
    "    type: string\n",
    "  plot_save_dir:\n",
    "    type: string\n",
    "  ms_threshold:\n",
    "    type: number\n",
    "  corr_threshold:\n",
    "    type: number\n",
    "  plot_heatmaps:\n",
    "    type: boolean\n",
    "  max_unique_threshold:\n",
    "    type: number\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ..\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python ${{inputs.data_prep_python_path}} \n",
    "  --subscription_id ${{inputs.subscription_id}} \n",
    "  --resource_group ${{inputs.resource_group}} \n",
    "  --workspace_name ${{inputs.workspace_name}}\n",
    "  --data_name ${{inputs.data_name}}\n",
    "  --plot_save_dir ${{inputs.plot_save_dir}}\n",
    "  --ms_threshold ${{inputs.ms_threshold}}\n",
    "  --corr_threshold ${{inputs.corr_threshold}}\n",
    "  --plot_heatmaps ${{inputs.plot_heatmaps}}\n",
    "  --max_unique_threshold ${{inputs.max_unique_threshold}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "component_register_dir = \"./component_registration\"\n",
    "os.makedirs(component_register_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {component_register_dir}/data_prep.py\n",
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "\n",
    "# Specify the directory containing the components\n",
    "#component_directory = os.path.abspath('./components')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(component_directory):\n",
    "#    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the components directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "\n",
    "# parent_directory = '../components/'\n",
    "# Loading the component from the yml file\n",
    "#loaded_component_prep = load_component(source=os.path.join(component_directory, \"data_prep.yaml\"))\n",
    "\n",
    "loaded_component_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep = ml_client.create_or_update(loaded_component_prep)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep.name} with Version {data_prep.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "# Check if the directories and files exist\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(component_directory):\n",
    "    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(data_prep_yaml_path):\n",
    "    print(f\"File '{data_prep_yaml_path}' does not exist. Please check the file path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Now, you have the absolute paths\n",
    "print(\"Absolute path to aml_config directory:\", aml_config_dir)\n",
    "print(\"Absolute path to components directory:\", component_directory)\n",
    "print(\"Absolute path to data_prep.yaml file:\", data_prep_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_preparation_py_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Specify the directory containing the components\n",
    "#component_directory = os.path.abspath('./components')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(component_directory):\n",
    "#    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the components directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "# Loading the component from the yml file\n",
    "data_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "\n",
    "cpu_compute_target, cpu_cluster = aml.get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target\n",
    "    if (cpu_cluster)\n",
    "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"first pipeline\",\n",
    ")\n",
    "def classification_pipeline(\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    workspace_name,\n",
    "    data_name,\n",
    "    plot_save_dir,\n",
    "    ms_threshold,\n",
    "    corr_threshold,\n",
    "    plot_heatmaps,\n",
    "    #columns_to_drop,\n",
    "    max_unique_threshold,\n",
    "    data_preparation_py_path\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep(\n",
    "        subscription_id = subscription_id,\n",
    "        resource_group = resource_group,\n",
    "        workspace_name = workspace_name,\n",
    "        data_name = data_name,\n",
    "        plot_save_dir = plot_save_dir,\n",
    "        ms_threshold = ms_threshold,\n",
    "        corr_threshold = corr_threshold,\n",
    "        plot_heatmaps = plot_heatmaps,\n",
    "        #columns_to_drop = columns_to_drop,\n",
    "        max_unique_threshold = max_unique_threshold,\n",
    "        data_preparation_py_path = data_preparation_py_path\n",
    "        \n",
    "    )\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.output_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "# Now, you have the absolute paths\n",
    "print(\"Absolute path to aml_config directory:\", aml_config_dir)\n",
    "print(\"Absolute path to components directory:\", component_directory)\n",
    "print(\"Absolute path to data_prep.yaml file:\", data_prep_python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = classification_pipeline(\n",
    "    subscription_id = \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "    resource_group = \"practice_resource\",\n",
    "    workspace_name = \"practice_workspace\",\n",
    "    data_name = 'ai4i2020.csv',\n",
    "    plot_save_dir = 'plots',\n",
    "    ms_threshold = 10,\n",
    "    corr_threshold = 0.8,\n",
    "    plot_heatmaps = True,\n",
    "    #columns_to_drop = columns_to_drop,\n",
    "    max_unique_threshold = 0.9, \n",
    "    data_preparation_py_path = data_preparation_py_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02cb41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de5384",
   "metadata": {},
   "source": [
    "\"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group\": \"practice_resource\",\n",
    "            \"workspace_name\": \"practice_workspace\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106467a",
   "metadata": {},
   "source": [
    "df = dp.drop_high_cardinality_features(df, max_unique_threshold=0.9)\n",
    "df = dp.drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True)\n",
    "df = dp.replace_missing_values(df, ms_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scripts_dir}/model_training.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/modules\")\n",
    "from data_prep import *\n",
    "from data_ingestion import *\n",
    "import argpase \n",
    "import mlflow\n",
    "\n",
    "#def a custom argument type for a list of strings\n",
    "def list_of_strings(arg):\n",
    "    return arg.split(',')\n",
    "\n",
    "def load_data(subscription_id:str, resource_group_name:str, workspace_name:str, data_path:str)\n",
    "def main():\n",
    "    #setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #add arguments\n",
    "    parser.add_argument('--subscription_id', dest='subscription_id',\n",
    "                       type=str, help=\"Azure subscription id\")\n",
    "    parser.add_argument('--resource_group', dest='resource_group',\n",
    "                       type=str, help=\"resource group name\")\n",
    "    parser.add_argument('--workspace_name', dest='workspace_name',\n",
    "                       type=str, help=\"workspace_name\")\n",
    "    parser.add_argument('--data_name', dest='data_name',\n",
    "                       type=str, help=\"data_path\")\n",
    "    parser.add_argument('--plot_save_dir', dest='plot_save_dir'\n",
    "                       type=str, help='Name of Parent Directory to store pipeline artefacts')\n",
    "    parser.add_argument('--ms_threshold', dest='ms_threshold',\n",
    "                       type=int, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--corr_threshold', dest='corr_threshold',\n",
    "                       type=float, help='The threshold for correlation above which features will be dropped')\n",
    "    parser.add_argument('--plot_heatmaps', dest='plot_heatmaps',\n",
    "                       type=bool, help='Whether to plot heatmaps before and after dropping (default is True)')\n",
    "    parser.add_argument('columns_to_drop', dest='columns_to_drop', \n",
    "                       type=list_of_strings, help='Single column name or a list of column names to be dropped')\n",
    "    parser.add_argument('--max_unique_threshold', dest='max_unique_threshold',\n",
    "                       type=float, help='The maximum allowed fraction of unique values in a column (default is 0.9)')\n",
    "    parser.add_argument('--output_data', dest='output_data',\n",
    "                       type=str)\n",
    "    \n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    # Load in the data from synapse DataLake Storage\n",
    "    raw_df = load_data(args.subscription_id, args.resource_group_name, args.workspace_name, args.data_name)\n",
    "    # Direct working directory to Artefact location\n",
    "    set_cwd_path(args.plot_save_dir)\n",
    "    \n",
    "    if args.training_run == 'True':\n",
    "        training_run = True\n",
    "    elif args.training_run == 'False':\n",
    "        training_run = False\n",
    "    else:\n",
    "        raise ValueError('Training Run Parameter must be \"True\" or \"False\"')\n",
    "        \n",
    "    df = check_missing_values(raw_df)\n",
    "    df = drop_high_cardinality_features(df, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.threshold, plot_heatmaps=True)\n",
    "    \n",
    "    # Reset directory back to initial working directory\n",
    "    set_cwd_path('..')\n",
    "    \n",
    "    mlflow.log_metric('Sample Size', df.shape[0])\n",
    "    \n",
    "    df.to_parquet(os.path.join(args.output_data, 'processed_df.parquet'), engine='pyarrow')\n",
    "    \n",
    "    # End Logging\n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f55348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sd.custom_train_test_split(df, target_column, test_size=0.2, random_state=None, time_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {components_dir}/data_prep.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17190ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ca15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee95ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d2233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <component>\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  subscription_id:\n",
    "    type: string\n",
    "  resource_group:\n",
    "    type: string\n",
    "  workspace_name:\n",
    "    type: string\n",
    "  data_name:\n",
    "    type: string\n",
    "  plot_save_dir:\n",
    "    type: string\n",
    "  ms_threshold:\n",
    "    type: number\n",
    "  corr_threshold:\n",
    "    type: number\n",
    "  plot_heatmaps:\n",
    "    type: boolean\n",
    "  columns_to_drop:\n",
    "    type: string\n",
    "  max_unique_threshold:\n",
    "    type: number\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python src/data_preparation.py \n",
    "  --subscription_id ${{inputs.subscription_id}} \n",
    "  --resource_group ${{inputs.resource_group}} \n",
    "  --workspace_name ${{inputs.workspace_name}}\n",
    "  --data_name ${{inputs.data_name}}\n",
    "  --plot_save_dir ${{inputs.plot_save_dir}}\n",
    "  --ms_threshold ${{inputs.ms_threshold}}\n",
    "  --corr_threshold ${{inputs.corr_threshold}}\n",
    "  --plot_heatmaps ${{inputs.plot_heatmaps}}\n",
    "  --columns_to_drop ${{inputs.columns_to_drop}}\n",
    "  --max_unique_threshold ${{inputs.max_unique_threshold}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f006f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {modules_dir}/aml_config.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "import json\n",
    "\n",
    "#def create_ml_client(subscription_id: str, resource_group: str, workspace_name: str, tenant_id: str = None):\n",
    "def create_ml_client():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create an Azure Machine Learning workspace client.\n",
    "\n",
    "    This function attempts to create an Azure Machine Learning workspace client using the provided parameters. If it fails\n",
    "    to create a client, it generates a new configuration file with the provided parameters and tries again.\n",
    "\n",
    "    Parameters:\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group (str): Azure resource group name.\n",
    "        workspace_name (str): Azure Machine Learning workspace name.\n",
    "        tenant_id (str, optional): Azure Active Directory tenant ID. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        azureml.core.Workspace: An Azure Machine Learning workspace client.\n",
    "    \"\"\"\n",
    "    # Create an Azure CLI credential\n",
    "    credentials = AzureCliCredential(tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')\n",
    "    \n",
    "    try:\n",
    "        # Try to create the Azure Machine Learning workspace client using provided parameters\n",
    "        ml_client = Workspace.from_config(auth=credentials)\n",
    "    except Exception as ex:\n",
    "        print(\"An error occurred while creating the AML client:\", str(ex))\n",
    "        print(\"Creating a new configuration file...\")\n",
    "\n",
    "        # Define the workspace configuration based on the provided parameters\n",
    "        client_config = {\n",
    "            \"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group_name\": \"victor_resource\",\n",
    "            \"workspace_name\": \"victor_workspace\",\n",
    "        }\n",
    "\n",
    "        # Write the configuration to a JSON file\n",
    "        config_path = \"../project_CAS/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            json.dump(client_config, fo)\n",
    "        \n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        ml_client = Workspace.from_config(path=config_path)\n",
    "    \n",
    "    return ml_client\n",
    "\n",
    "def get_compute(ml_client, name:str, vm_size:str, min_instance:int, max_instances:int):\n",
    "    # specify aml compute name.\n",
    "    cpu_compute_target = name\n",
    "    \n",
    "    try:\n",
    "        cpu_cluster = ml_client.compute_targets[cpu_compute_target]\n",
    "        print(f'Using existing compute target: {cpu_compute_target}')\n",
    "    except KeyError:\n",
    "        print(f\"Creating a new cpu compute target: {cpu_compute_target}...\")\n",
    "        compute_config = AmlCompute.provisioning_configuration(\n",
    "            vm_size=vm_size,\n",
    "            min_nodes=min_instance,\n",
    "            max_nodes=max_instances\n",
    "        )\n",
    "        cpu_cluster = AmlCompute.create(ml_client, name=cpu_compute_target, provisioning_configuration=compute_config)\n",
    "        cpu_cluster.wait_for_completion(show_output=True)\n",
    "        \n",
    "    return cpu_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cadb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"./modules\")\n",
    "import aml_config as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = ac.create_ml_client(subscription_id=\"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\", resource_group=\"victor_resource\", workspace_name=\"victor_workspace\", tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.get_compute(ml_client, name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8.*\n",
    "  - pip=23.2.*\n",
    "  - pip:\n",
    "    - numpy==1.22.*\n",
    "    - mlflow== 2.4.1\n",
    "    - azureml-core==1.53.*\n",
    "    - azureml-defaults==1.53.*\n",
    "    - mlflow==2.6.*\n",
    "    - scikit-learn==1.3.*\n",
    "    - azure-ai-ml==1.9.0\n",
    "    - requests==2.31.*\n",
    "    - azure-identity==1.14.0\n",
    "    - scipy==1.7.1\n",
    "    - pandas==1.4.4\n",
    "    - shap==0.42.1\n",
    "    - joblib==1.3.2\n",
    "    - seaborn==0.11.2\n",
    "    - matplotlib==3.4.*\n",
    "    - shapely==2.0.*\n",
    "    - scikit-optimize==0.9.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
