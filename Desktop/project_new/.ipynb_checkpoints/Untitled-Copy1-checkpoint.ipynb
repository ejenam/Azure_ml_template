{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b494a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4ce9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modules_dir = \"./modules\"\n",
    "os.makedirs(modules_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466bb32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_prep.py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Directory to save the plots\n",
    "plot_save_dir = 'plots'\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def check_missing_values(df, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Check and visualize missing values in a DataFra: str=me.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to check for missing values.\n",
    "        artifact_save_dir (str): Directory to save the heatmap plot and other artifacts.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # Check for missing values and compute the count of missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Plot a heatmap of missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "\n",
    "    # List the number of missing values in each column\n",
    "    log.info(\"Number of missing values in each column:\")\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            log.info(f\"Column '{column}' had {count} missing values.\")\n",
    "\n",
    "    # Create the artifact_save_dir directory if it doesn't exist\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "\n",
    "    # Save the heatmap plot as an image in the specified directory\n",
    "    plot_name = 'missing_values_heatmap'\n",
    "    plot_save_path = os.path.join(artifact_save_dir, f\"{plot_name}.png\")\n",
    "    plt.savefig(plot_save_path)\n",
    "    log.info(f'{plot_name} saved at: {plot_save_path}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def replace_missing_values(df, ms_threshold: int, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Replace missing values in a DataFrame using interpolation and iterative imputation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing missing values.\n",
    "        ms_threshold (int): Threshold to switch between interpolation and iterative imputer.\n",
    "        artifact_save_dir (str, optional): Directory to save artifacts (e.g., logs) (default: None).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values replaced.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    # If an artifact_save_dir is specified, configure the logger to save logs to that directory\n",
    "    if artifact_save_dir:\n",
    "        log_filename = 'replace_missing_values.log'\n",
    "        log_filepath = os.path.join(artifact_save_dir, log_filename)\n",
    "\n",
    "        # Configure the logger\n",
    "        logging.basicConfig(filename=log_filepath, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Threshold to switch between interpolation and iterative imputer\n",
    "    interpolation_threshold = ms_threshold\n",
    "\n",
    "    # Count the missing values in each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # List to store column names that need imputation\n",
    "    columns_to_impute = []\n",
    "\n",
    "    # Identify columns where the gap between missing values is less than the threshold\n",
    "    for column, count in missing_values.items():\n",
    "        if count > 0:\n",
    "            indices = df[column].index[df[column].isnull()]\n",
    "            differences = np.diff(indices)\n",
    "            if all(diff <= interpolation_threshold for diff in differences):\n",
    "                columns_to_impute.append(column)\n",
    "\n",
    "    # Separate columns for interpolation and iterative imputer\n",
    "    columns_to_interpolate = [col for col in columns_to_impute if col not in columns_to_impute]\n",
    "    columns_to_iterative_impute = [col for col in columns_to_impute if col in columns_to_impute]\n",
    "\n",
    "    # Replace missing values with interpolation\n",
    "    if len(columns_to_interpolate) > 0:\n",
    "        imputer = SimpleImputer(strategy='nearest')\n",
    "        df[columns_to_interpolate] = imputer.fit_transform(df[columns_to_interpolate])\n",
    "        for column in columns_to_interpolate:\n",
    "            log.info(f\"Imputed '{column}' using 'nearest' strategy.\")\n",
    "\n",
    "    # Replace missing values with iterative imputer\n",
    "    if len(columns_to_iterative_impute) > 0:\n",
    "        imputer = IterativeImputer()\n",
    "        df[columns_to_iterative_impute] = imputer.fit_transform(df[columns_to_iterative_impute])\n",
    "        for column in columns_to_iterative_impute:\n",
    "            log.info(f\"Imputed '{column}' using 'iterative' strategy.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on Spearman correlation coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the dataset.\n",
    "    - corr_threshold: The threshold for correlation above which features will be dropped (default is 0.8).\n",
    "    - plot_heatmaps: Whether to plot heatmaps before and after dropping (default is True).\n",
    "    - artifact_save_dir: Directory to save the correlation heatmap plots (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the highly correlated features dropped.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    if artifact_save_dir and not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Calculate the correlation matrix (Spearman by default in pandas)\n",
    "    corr_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Plot the correlation heatmap before dropping\n",
    "        fig_before = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (Before Dropping)\")\n",
    "        sns_plot_before = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "            log.info(\"Correlation heatmap (Before Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Create a set to store the columns to drop\n",
    "    columns_to_drop = set()\n",
    "    \n",
    "    # Create a list to store the names of the dropped columns\n",
    "    dropped_columns = []\n",
    "    \n",
    "    # Iterate through the columns and identify highly correlated features\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            if col1 != col2 and abs(corr_matrix.loc[col1, col2]) >= corr_threshold:\n",
    "                # Check if col1 or col2 should be dropped based on their mean correlation\n",
    "                mean_corr_col1 = corr_matrix.loc[col1, :].drop(col1).abs().mean()\n",
    "                mean_corr_col2 = corr_matrix.loc[col2, :].drop(col2).abs().mean()\n",
    "                \n",
    "                if mean_corr_col1 > mean_corr_col2:\n",
    "                    columns_to_drop.add(col1)\n",
    "                    dropped_columns.append(col1)\n",
    "                else:\n",
    "                    columns_to_drop.add(col2)\n",
    "                    dropped_columns.append(col2)\n",
    "    \n",
    "    # Drop the highly correlated features from the DataFrame\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Calculate the correlation matrix after dropping\n",
    "        corr_matrix_after_drop = df.corr(method='spearman')\n",
    "        \n",
    "        # Plot the correlation heatmap after dropping\n",
    "        fig_after = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (After Dropping)\")\n",
    "        sns_plot_after = sns.heatmap(corr_matrix_after_drop, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "            log.info(\"Correlation heatmap (After Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"Dropped columns: %s\", dropped_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_columns(data, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drop selected columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "    - columns_to_drop: Single column name or a list of column names to be dropped.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the specified columns dropped.\n",
    "    \"\"\"\n",
    "    if isinstance(columns_to_drop, str):\n",
    "        # If a single column name is provided, convert it to a list\n",
    "        columns_to_drop = [columns_to_drop]\n",
    "\n",
    "    # Drop the specified columns from the DataFrame\n",
    "    df = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"bad_columns_dropped are/is: %s\", columns_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_high_cardinality_features(df, max_unique_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Drop high cardinality features (columns) from a DataFrame based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        max_unique_threshold (float): The maximum allowed fraction of unique values in a column (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with high cardinality columns dropped.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"Input DataFrame 'df' cannot be None.\")\n",
    "        \n",
    "    # Calculate the maximum number of allowed unique values for each column\n",
    "    max_unique_values = len(df) * max_unique_threshold\n",
    "    \n",
    "    # Identify and drop columns with unique values exceeding the threshold\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > max_unique_values]\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    if high_cardinality_columns:\n",
    "        log.info(f\"Dropped high cardinality columns: {', '.join(high_cardinality_columns)}\")\n",
    "    \n",
    "    df_dropped = df.drop(columns=high_cardinality_columns)\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "def select_categorical_columns(data):\n",
    "    \"\"\"\n",
    "    Select categorical columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - A list of column names that are categorical.\n",
    "    \"\"\"\n",
    "    categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    return categorical_columns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c9664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./modules\")\n",
    "import data_prep as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcea90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sd.custom_train_test_split(df, target_column, test_size=0.2, random_state=None, time_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "892910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c53266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af74f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (23.9.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.3.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-optimize) (1.3.2)\n",
      "Requirement already satisfied: PyYAML in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c1d53bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./modules/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5a92dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/tune_train_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/tune_train_test.py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, model_prefix:str, param_grid=None, random_search=False, bayesian_search=False, n_iter=10, random_seed=101):\n",
    "    \"\"\"\n",
    "    Train a Histogram Gradient Boosting Classifier and tune its hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_test, y_test: Testing data and labels.\n",
    "    - model_prefix: Prefix for model artifacts.\n",
    "    - param_grid: Hyperparameter grid to search (default is None).\n",
    "    - random_search: Whether to use random search instead of grid search (default is False).\n",
    "    - bayesian_search: Whether to use Bayesian hyperparameter search (default is False).\n",
    "    - n_iter: Number of parameter settings that are sampled (only for random_search or bayesian_search).\n",
    "\n",
    "    Returns:\n",
    "    - Trained model, best hyperparameters, and test accuracy.\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    # Create a Histogram Gradient Boosting Classifier\n",
    "    clf = HistGradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # Combine preprocessing and classifier into a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    if not bayesian_search:\n",
    "        # Define hyperparameters for grid search or random search\n",
    "        hyperparameters = {\n",
    "            'classifier__max_iter': [100, 200, 300],  # Adjust the values as needed\n",
    "            'classifier__learning_rate': [0.001, 0.01, 0.1],  # Adjust the values as needed\n",
    "            'classifier__max_depth': [3, 4, 5],  # Adjust the values as needed\n",
    "            'classifier__l2_regularization': [0.0, 0.1, 0.2]  # Adjust the values as needed\n",
    "        }\n",
    "\n",
    "        if random_search:\n",
    "            # Use RandomizedSearchCV\n",
    "            search = RandomizedSearchCV(pipeline, param_distributions=hyperparameters, n_iter=n_iter, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "        else:\n",
    "            # Use GridSearchCV\n",
    "            search = GridSearchCV(pipeline, param_grid=hyperparameters, scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "    else:\n",
    "        # Use Bayesian hyperparameter search with BayesSearchCV\n",
    "        param_grid = {\n",
    "            'classifier__max_iter': (100, 300),\n",
    "            'classifier__learning_rate': (0.001, 0.1),\n",
    "            'classifier__max_depth': (3, 5),\n",
    "            'classifier__l2_regularization': (0.0, 0.2)\n",
    "        }\n",
    "\n",
    "        search = BayesSearchCV(pipeline, param_grid, n_iter=n_iter, cv=TimeSeriesSplit(n_splits=3), scoring='accuracy', n_jobs=-1, random_state=random_seed)\n",
    "\n",
    "    # Fit the search to the training data\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and the best estimator (trained model)\n",
    "    best_params = search.best_params_\n",
    "    best_estimator = search.best_estimator_\n",
    "    \n",
    "    log.info('Parameters chosen are:')\n",
    "    log.info(best_params)\n",
    "    \n",
    "    log.info('The best estimator is:')\n",
    "    log.info(best_estimator)\n",
    "    \n",
    "    # Evaluate the best model on the test data\n",
    "   # y_pred = best_estimator.predict(X_test)\n",
    "   # test_accuracy = accuracy_score(y_test, y_pred)\n",
    "   # log.info(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "    \n",
    "    # Save the best model to a file\n",
    "    model_filename = f'{model_prefix}_best_model.joblib'\n",
    "    joblib.dump(best_estimator, model_filename)\n",
    "    \n",
    "    # Save best hyperparameters to a JSON file\n",
    "    hyperparameters_filename = f'{model_prefix}_hyperparameters.json'\n",
    "    log.info(f'Saving best hyperparameters for {model_prefix} as {hyperparameters_filename}')\n",
    "    with open(hyperparameters_filename, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "        \n",
    "    return best_params, hyperparameters_filename\n",
    "\n",
    "def train_model(X_train, y_train, model_name:str, hyperparam: dict=None, hyperparam_filename: str=None):\n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_train.select_dtypes(include=['category', 'object']).columns)\n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    if hyperparam_filename is not None:\n",
    "        log.info(f'Loading in hyperparameters: {hyperparam_filename}')\n",
    "        with open(hyperparam_filename, 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "    elif hyperparam is not None:\n",
    "        best_params = hyperparam\n",
    "    else:\n",
    "        raise ValueError('Either hyperparam or hyperparam_filename must be assigned')\n",
    "    \n",
    "    # Create and train the model with the specified hyperparameters\n",
    "    log.info('Training Model')\n",
    "    trained_model = HistGradientBoostingClassifier(class_weight='balanced',\n",
    "        max_iter=best_params['classifier__max_iter'],\n",
    "        learning_rate=best_params['classifier__learning_rate'],\n",
    "        max_depth=best_params['classifier__max_depth'],\n",
    "        l2_regularization=best_params['classifier__l2_regularization'],\n",
    "        random_state=10\n",
    "    )\n",
    "    trained_model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    log.info(f'Saving {model_name}')\n",
    "    joblib.dump(trained_model, model_name)\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "def predict_model(trained_model, X_test, inference_col_name):\n",
    "    \"\"\"\n",
    "    Predict using a trained machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_model: The trained machine learning model.\n",
    "    - X_test: The test dataset on which to make predictions.\n",
    "    - inference_col_name: The name of the column to store predictions in the inference DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - inference_df: The DataFrame containing predictions.\n",
    "    - inference_col_name: The name of the column where predictions are stored.\n",
    "    - predictions: The predictions made by the model.\n",
    "    \"\"\"\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "    \n",
    "    # Get the one-hot encoded feature names\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    cat_feature_names = list(ohe.get_feature_names_out(input_features=categorical_features))\n",
    "    \n",
    "    # Combine the one-hot encoded feature names and non-categorical column names\n",
    "    all_column_names = cat_feature_names + list(X_test.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    \n",
    "    # Convert X_test_transformed to a DataFrame with appropriate column names\n",
    "    inference_df = pd.DataFrame(X_test_transformed, columns=all_column_names)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    predictions = trained_model.predict(X_test_transformed)\n",
    "    \n",
    "    # Add predictions to the DataFrame with the specified column name\n",
    "    inference_df[inference_col_name] = predictions\n",
    "    \n",
    "    return inference_df, inference_col_name, predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e2902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34876284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/evaluation.py\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Directory to save the plots\n",
    "plot_save_dir = 'plots'\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          model_name: str,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          artifact_save_dir: str):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix.\n",
    "\n",
    "    :param y_true: True labels of the data.\n",
    "    :param y_pred: Predicted labels of the data.\n",
    "    :param classes: List of class labels (e.g., ['Class 0', 'Class 1']).\n",
    "    :param model_name: Name of the model for plot naming.\n",
    "    :param normalize: If True, normalize the confusion matrix.\n",
    "    :param title: Title of the plot.\n",
    "    :param cmap: Colormap for the plot.\n",
    "    :param artifact_save_dir: Directory where artifacts including plots will be saved.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "\n",
    "    # Create figure and axis\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    # Customize plot labels and appearance\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Artifact save directory for plots\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Save the heatmap plot as an image\n",
    "    plot_name = f'{model_name}_confusion_matrix'  # Set the desired plot name\n",
    "    plot_save_path = os.path.join(artifact_save_dir, f\"{plot_name}.png\")  # Format the file name\n",
    "    plt.savefig(plot_save_path)\n",
    "    log.info(f'{plot_name} saved')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shap_feature_importance(X_test, model_name, n_features, artifact_save_dir: str):\n",
    "    \n",
    "    model_prefix = os.path.splitext(model_name)[0]\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        log.info(f'Creating {artifact_save_dir} directory in {os.getcwd()}')\n",
    "        os.makedirs(artifact_save_dir)\n",
    "        \n",
    "    log.info(f'Loading {model_name}')\n",
    "    model = joblib.load(model_name)\n",
    "    \n",
    "    log.info('Generating SHAP Values')\n",
    "    \n",
    "    # Convert X_test to a pandas DataFrame\n",
    "    X_test_df = pd.DataFrame(X_test)  # Assuming X_test is a 2D array\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test_df.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test_df)\n",
    "    \n",
    "    # Generate SHAP values using the transformed data\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_transformed)\n",
    "    \n",
    "    log.info('Saving SHAP Plot to Artifact Directory')\n",
    "    plt.clf()\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)) +\n",
    "        list(X_test_df.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    )\n",
    "    shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    # Artifact save directory for plots\n",
    "    plot_save_dir = os.path.join(artifact_save_dir, 'shap_summary_plots')\n",
    "    if not os.path.exists(plot_save_dir):\n",
    "        os.makedirs(plot_save_dir)\n",
    "    \n",
    "    plot_name = os.path.join(plot_save_dir, f'{model_prefix}_SHAP_summary.png')\n",
    "    plt.savefig(plot_name)\n",
    "    log.info(f'{plot_name} saved.')\n",
    "    \n",
    "    log.info(f'Extracting Top {n_features} Important features for the model')\n",
    "    feature_importances = np.abs(shap_values).mean(axis=0)\n",
    "    feature_importances_dict = dict(zip(feature_names, feature_importances))\n",
    "    sorted_features = sorted(feature_importances_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    log.info(f'Top {n_features} Important features are:')\n",
    "    top_n_feature = sorted_features[:n_features]\n",
    "    top_n_feature_names = [feature[0] for feature in top_n_feature]  # Extract feature names\n",
    "    top_n_feature_importances = [feature[1] for feature in top_n_feature]  # Extract importances\n",
    "    \n",
    "    log.info(', '.join([f'{feature}: {importance}' for feature, importance in top_n_feature]))\n",
    "    \n",
    "    return plot_name, shap_values, top_n_feature_names, top_n_feature_importances\n",
    "\n",
    "\n",
    "def shap_dependence_plots(X_test: pd.DataFrame, features: list, model_name: str, shap_values: np.array, artifact_save_dir: str):\n",
    "    model_prefix = os.path.splitext(model_name)[0]\n",
    "    if not os.path.exists(artifact_save_dir):\n",
    "        log.info(f'Creating {artifact_save_dir} directory in {os.getcwd()}')\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    log.info(f'Loading {model_name}')\n",
    "    model = joblib.load(model_name)\n",
    "    \n",
    "    log.info('Generating SHAP Values')\n",
    "    \n",
    "    # Convert X_test to a pandas DataFrame\n",
    "    X_test_df = pd.DataFrame(X_test)  # Assuming X_test is a 2D array\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = list(X_test_df.select_dtypes(include=['category', 'object']).columns)\n",
    "    \n",
    "    # Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep non-categorical columns as-is\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the test data\n",
    "    X_test_transformed = preprocessor.fit_transform(X_test_df)\n",
    "    \n",
    "    # Generate feature names\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)) +\n",
    "        list(X_test_df.select_dtypes(exclude=['category', 'object']).columns)\n",
    "    )\n",
    "\n",
    "    for index, feature_name in enumerate(features):\n",
    "        # Find the corresponding integer index for the feature name\n",
    "        try:\n",
    "            feature_index = feature_names.index(feature_name)\n",
    "        except ValueError:\n",
    "            log.warning(f'Feature {feature_name} not found in feature_names. Skipping.')\n",
    "            continue\n",
    "            \n",
    "        # Artifact save directory for plots\n",
    "        plot_save_dir = os.path.join(artifact_save_dir, 'shap_dependence_plots')\n",
    "        if not os.path.exists(plot_save_dir):\n",
    "            os.makedirs(plot_save_dir)\n",
    "        \n",
    "        plot_name = os.path.join(plot_save_dir, f'{model_prefix}_feature{index}_SHAP_dependence.png') \n",
    "        \n",
    "        # Clear the previous plot\n",
    "        plt.clf()\n",
    "        shap.dependence_plot(feature_index, shap_values, X_test_transformed, \n",
    "                             feature_names=feature_names, show=False)  # Pass feature names for labeling\n",
    "        # Save the SHAP dependence plot as an image\n",
    "        fig = plt.gcf()\n",
    "        plt.savefig(plot_name, dpi=150, bbox_inches='tight')\n",
    "        log.info(f'{plot_name} saved.')\n",
    "        \n",
    "    log.info(f'All SHAP dependence plots saved in {plot_save_dir}')\n",
    "    \n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def evaluate_classification_models(model_name, predictions, true_labels, target_names, plot_classification_report=False, artifact_save_dir: str = 'artifacts'):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model and generate a classification report.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model for plot naming.\n",
    "        predictions (array-like): Model predictions (predicted class labels).\n",
    "        true_labels (array-like): True class labels.\n",
    "        target_names (list): e.g ['Class 0', 'Class 1']\n",
    "        plot_classification_report (bool): Whether to plot the classification report (default: False).\n",
    "        artifact_save_dir (str): Directory where artifacts including plots will be saved.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed metrics.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    evaluation_results['accuracy'] = accuracy\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    evaluation_results['precision'] = precision\n",
    "\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    evaluation_results['recall'] = recall\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    evaluation_results['f1'] = f1\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels, predictions)\n",
    "        evaluation_results['roc_auc'] = roc_auc\n",
    "    except ValueError:\n",
    "        # roc_auc_score may not work for multiclass classification\n",
    "        evaluation_results['roc_auc'] = None\n",
    "\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    evaluation_results['confusion_matrix'] = confusion\n",
    "\n",
    "    if plot_classification_report:\n",
    "        # Generate the classification report\n",
    "        report = classification_report(\n",
    "            true_labels, predictions, target_names=target_names, output_dict=True\n",
    "        )\n",
    "\n",
    "        # Convert the classification report to a DataFrame for easy plotting\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "        # Plot the classification report as a heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(report_df.iloc[:-3, :-1], annot=True, cmap='Blues', fmt=\".2f\", cbar=False)\n",
    "        plt.title('Classification Report')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Metrics')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        model_prefix = os.path.splitext(model_name)[0]\n",
    "        plot_name = os.path.join(artifact_save_dir, f'{model_prefix}_Classification_Report.png')\n",
    "        plt.savefig(plot_name)\n",
    "        log.info(f'{plot_name} saved.')\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "        # Add classification report metrics to the evaluation results\n",
    "        for metric, values in report.items():\n",
    "            if isinstance(values, dict):\n",
    "                for class_name, value in values.items():\n",
    "                    metric_name = f\"{metric}_{class_name}\"\n",
    "                    evaluation_results[metric_name] = value\n",
    "    log.info(evaluation_results)\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d47704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "INFO:tune_train_test:Parameters chosen are:\n",
      "INFO:tune_train_test:OrderedDict([('classifier__l2_regularization', 0.04673448075803774), ('classifier__learning_rate', 0.01985513222581879), ('classifier__max_depth', 5), ('classifier__max_iter', 120)])\n",
      "INFO:tune_train_test:The best estimator is:\n",
      "INFO:tune_train_test:Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('cat', OneHotEncoder(),\n",
      "                                                  ['Type'])])),\n",
      "                ('classifier',\n",
      "                 HistGradientBoostingClassifier(l2_regularization=0.04673448075803774,\n",
      "                                                learning_rate=0.01985513222581879,\n",
      "                                                max_depth=5, max_iter=120,\n",
      "                                                random_state=42))])\n",
      "INFO:tune_train_test:Saving best hyperparameters for test as test_hyperparameters.json\n",
      "INFO:tune_train_test:Loading in hyperparameters: test_hyperparameters.json\n",
      "INFO:tune_train_test:Training Model\n",
      "INFO:tune_train_test:Saving HGBR.pkl\n"
     ]
    }
   ],
   "source": [
    "import tune_train_test as tt\n",
    "best_params, hyperparameters_filename= tt.hyperparameter_tuning(X_train, y_train, 'test', bayesian_search=True, n_iter=30, random_seed=42)\n",
    "\n",
    "trained_model = tt.train_model(X_train, y_train, 'HGBR.pkl', hyperparam_filename=hyperparameters_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da927075",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df, inference_col_name, predictions = tt.predict_model(trained_model, X_test, 'predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99de5840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type_H</th>\n",
       "      <th>Type_L</th>\n",
       "      <th>Type_M</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>302.5</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301.5</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>297.7</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.5</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296.5</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>301.7</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.4</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.5</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298.9</td>\n",
       "      <td>1583.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type_H  Type_L  Type_M  Air temperature [K]  Rotational speed [rpm]  \\\n",
       "0        0.0     0.0     1.0                302.5                  1507.0   \n",
       "1        0.0     1.0     0.0                301.5                  1785.0   \n",
       "2        0.0     1.0     0.0                299.0                  1929.0   \n",
       "3        0.0     0.0     1.0                297.7                  1365.0   \n",
       "4        0.0     1.0     0.0                302.5                  1463.0   \n",
       "...      ...     ...     ...                  ...                     ...   \n",
       "1995     0.0     1.0     0.0                296.5                  1340.0   \n",
       "1996     0.0     0.0     1.0                301.7                  1625.0   \n",
       "1997     1.0     0.0     0.0                300.4                  1416.0   \n",
       "1998     0.0     1.0     0.0                302.5                  1497.0   \n",
       "1999     0.0     1.0     0.0                298.9                  1583.0   \n",
       "\n",
       "      Tool wear [min]  TWF  HDF  PWF  OSF  RNF  predictions  \n",
       "0               201.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "1               120.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "2               180.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "3                36.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "4               189.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "...               ...  ...  ...  ...  ...  ...          ...  \n",
       "1995             89.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "1996            121.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "1997             66.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "1998            123.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "1999            230.0  0.0  0.0  0.0  0.0  0.0            0  \n",
       "\n",
       "[2000 rows x 12 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f62b0b",
   "metadata": {},
   "source": [
    "import evaluation as e\n",
    "classes = [0, 1]\n",
    "e.plot_confusion_matrix(y_test, predictions, classes,\n",
    "                        model_name = 'model',\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)\n",
    "plot_name, shap_values, top_n_feature_names, top_n_feature_importances = e.shap_feature_importance(X_test, 'HGBR.pkl', n_features=5)\n",
    "e.shap_dependence_plots(X_test, top_n_feature_names, 'HGBR.pkl', shap_values, plot_save_dir='plots')\n",
    "true_labels = y_test.to_list()\n",
    "target_names=['Class 0', 'Class 1']\n",
    "e.evaluate_classification_models('HGBR.pkl', predictions, true_labels, target_names, plot_classification_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "25303680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azureml-sdk in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (1.48.0)\n",
      "Collecting azureml-sdk\n",
      "  Downloading azureml_sdk-1.53.0-py3-none-any.whl (2.7 kB)\n",
      "Collecting azureml-train-automl-client~=1.53.0\n",
      "  Downloading azureml_train_automl_client-1.53.0-py3-none-any.whl (137 kB)\n",
      "     |████████████████████████████████| 137 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-dataset-runtime[fuse]~=1.53.0\n",
      "  Downloading azureml_dataset_runtime-1.53.0-py3-none-any.whl (2.3 kB)\n",
      "Collecting azureml-train-core~=1.53.0\n",
      "  Downloading azureml_train_core-1.53.0-py3-none-any.whl (8.6 MB)\n",
      "     |████████████████████████████████| 8.6 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-core~=1.53.0\n",
      "  Downloading azureml_core-1.53.0-py3-none-any.whl (3.3 MB)\n",
      "     |████████████████████████████████| 3.3 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-pipeline~=1.53.0\n",
      "  Downloading azureml_pipeline-1.53.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pkginfo in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.2.7)\n",
      "Requirement already satisfied: msrest<=0.7.1,>=0.5.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.7.1)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.4.8)\n",
      "Requirement already satisfied: SecretStorage<4.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.3.3)\n",
      "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.5.1)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.1.28)\n",
      "Requirement already satisfied: PyJWT<3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.1.0)\n",
      "Collecting azure-mgmt-network==21.0.1\n",
      "  Downloading azure_mgmt_network-21.0.1-py3-none-any.whl (8.9 MB)\n",
      "     |████████████████████████████████| 8.9 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: azure-core<2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.29.2)\n",
      "Requirement already satisfied: azure-mgmt-storage<=21.0.0,>=16.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (20.1.0)\n",
      "Requirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.26.0)\n",
      "Requirement already satisfied: docker<7.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (6.0.1)\n",
      "Requirement already satisfied: argcomplete<3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied: paramiko<4.0.0,>=2.0.8 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.12.0)\n",
      "Requirement already satisfied: contextlib2<22.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.23 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.26.7)\n",
      "Requirement already satisfied: pyopenssl<24.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.0.0)\n",
      "Requirement already satisfied: knack~=0.10.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.10.1)\n",
      "Requirement already satisfied: humanfriendly<11.0,>=4.7 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.0)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<11.0.0,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.1.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization<4,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (3.0.0)\n",
      "Requirement already satisfied: pytz in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2021.3)\n",
      "Requirement already satisfied: msal-extensions<=1.0.0,>=0.3.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0.0)\n",
      "Requirement already satisfied: packaging<=23.0,>=20.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.3)\n",
      "Requirement already satisfied: azure-mgmt-resource<=22.0.0,>=15.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (21.2.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.8.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.15.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.21.0)\n",
      "Requirement already satisfied: pathspec<1.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.7.0)\n",
      "Requirement already satisfied: backports.tempfile in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry<11,>=8.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (10.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (1.0.1)\n",
      "Requirement already satisfied: jsonpickle<4.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (2.2.0)\n",
      "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.6.4)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-sdk) (0.61.1)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-mgmt-network==21.0.1->azureml-core~=1.53.0->azureml-sdk) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-core<2.0.0->azureml-core~=1.53.0->azureml-sdk) (4.7.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azure-core<2.0.0->azureml-core~=1.53.0->azureml-sdk) (1.16.0)\n",
      "Requirement already satisfied: pyarrow<=11.0.0,>=0.17.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (9.0.0)\n",
      "Collecting azureml-dataprep<4.14.0a,>=4.12.0a\n",
      "  Downloading azureml_dataprep-4.12.4-py3-none-any.whl (38.2 MB)\n",
      "     |████████████████████████████████| 38.2 MB 61 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fusepy<4.0.0,>=3.0.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (3.0.1)\n",
      "Requirement already satisfied: azure-identity>=1.7.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.12.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azureml-dataprep-rslex~=2.19.5dev0\n",
      "  Downloading azureml_dataprep_rslex-2.19.5-cp39-cp39-macosx_10_9_x86_64.whl (18.3 MB)\n",
      "     |████████████████████████████████| 18.3 MB 638 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (4.19.0)\n",
      "Requirement already satisfied: dotnetcore2<4.0.0,>=3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (3.1.23)\n",
      "Requirement already satisfied: cloudpickle<3.0.0,>=1.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied: azureml-dataprep-native<39.0.0,>=38.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (38.0.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (6.0)\n",
      "Collecting azureml-pipeline-core~=1.53.0\n",
      "  Downloading azureml_pipeline_core-1.53.0-py3-none-any.whl (313 kB)\n",
      "     |████████████████████████████████| 313 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azureml-pipeline-steps~=1.53.0\n",
      "  Downloading azureml_pipeline_steps-1.53.0-py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting azureml-telemetry~=1.53.0\n",
      "  Downloading azureml_telemetry-1.53.0-py3-none-any.whl (30 kB)\n",
      "Collecting azureml-automl-core~=1.53.0\n",
      "  Downloading azureml_automl_core-1.53.0-py3-none-any.whl (248 kB)\n",
      "     |████████████████████████████████| 248 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: applicationinsights in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from azureml-telemetry~=1.53.0->azureml-train-automl-client~=1.53.0->azureml-sdk) (0.11.10)\n",
      "Collecting azureml-train-restclients-hyperdrive~=1.53.0\n",
      "  Downloading azureml_train_restclients_hyperdrive-1.53.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.53.0->azureml-sdk) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.53.0->azureml-sdk) (2.20)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from docker<7.0.0->azureml-core~=1.53.0->azureml-sdk) (1.5.1)\n",
      "Requirement already satisfied: distro>=1.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from dotnetcore2<4.0.0,>=3.0.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.8.0)\n",
      "Requirement already satisfied: tabulate in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from knack~=0.10.0->azureml-core~=1.53.0->azureml-sdk) (0.9.0)\n",
      "Requirement already satisfied: pygments in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from knack~=0.10.0->azureml-core~=1.53.0->azureml-sdk) (2.10.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msal-extensions<=1.0.0,>=0.3.0->azureml-core~=1.53.0->azureml-sdk) (2.7.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (0.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (2022.12.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (1.3.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.53.0->azureml-sdk) (0.4.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from packaging<=23.0,>=20.0->azureml-core~=1.53.0->azureml-sdk) (3.0.4)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.53.0->azureml-sdk) (4.0.1)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.53.0->azureml-sdk) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from pyarrow<=11.0.0,>=0.17.0->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (1.22.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-sdk) (3.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-sdk) (1.7.1)\n",
      "Requirement already satisfied: jeepney>=0.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from SecretStorage<4.0.0->azureml-core~=1.53.0->azureml-sdk) (0.8.0)\n",
      "Requirement already satisfied: backports.weakref in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from backports.tempfile->azureml-core~=1.53.0->azureml-sdk) (1.0.post1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (0.30.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (0.9.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml-dataset-runtime[fuse]~=1.53.0->azureml-sdk) (23.1.0)\n",
      "Installing collected packages: azureml-dataprep-rslex, azure-mgmt-network, azureml-dataprep, azureml-core, azureml-train-restclients-hyperdrive, azureml-telemetry, azureml-dataset-runtime, azureml-train-core, azureml-automl-core, azureml-train-automl-client, azureml-pipeline-core, azureml-pipeline-steps, azureml-pipeline, azureml-sdk\n",
      "  Attempting uninstall: azureml-dataprep-rslex\n",
      "    Found existing installation: azureml-dataprep-rslex 2.15.2\n",
      "    Uninstalling azureml-dataprep-rslex-2.15.2:\n",
      "      Successfully uninstalled azureml-dataprep-rslex-2.15.2\n",
      "  Attempting uninstall: azureml-dataprep\n",
      "    Found existing installation: azureml-dataprep 4.8.6\n",
      "    Uninstalling azureml-dataprep-4.8.6:\n",
      "      Successfully uninstalled azureml-dataprep-4.8.6\n",
      "  Attempting uninstall: azureml-core\n",
      "    Found existing installation: azureml-core 1.48.0\n",
      "    Uninstalling azureml-core-1.48.0:\n",
      "      Successfully uninstalled azureml-core-1.48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: azureml-train-restclients-hyperdrive\n",
      "    Found existing installation: azureml-train-restclients-hyperdrive 1.48.0\n",
      "    Uninstalling azureml-train-restclients-hyperdrive-1.48.0:\n",
      "      Successfully uninstalled azureml-train-restclients-hyperdrive-1.48.0\n",
      "  Attempting uninstall: azureml-telemetry\n",
      "    Found existing installation: azureml-telemetry 1.48.0\n",
      "    Uninstalling azureml-telemetry-1.48.0:\n",
      "      Successfully uninstalled azureml-telemetry-1.48.0\n",
      "  Attempting uninstall: azureml-dataset-runtime\n",
      "    Found existing installation: azureml-dataset-runtime 1.48.0\n",
      "    Uninstalling azureml-dataset-runtime-1.48.0:\n",
      "      Successfully uninstalled azureml-dataset-runtime-1.48.0\n",
      "  Attempting uninstall: azureml-train-core\n",
      "    Found existing installation: azureml-train-core 1.48.0\n",
      "    Uninstalling azureml-train-core-1.48.0:\n",
      "      Successfully uninstalled azureml-train-core-1.48.0\n",
      "  Attempting uninstall: azureml-automl-core\n",
      "    Found existing installation: azureml-automl-core 1.48.0\n",
      "    Uninstalling azureml-automl-core-1.48.0:\n",
      "      Successfully uninstalled azureml-automl-core-1.48.0\n",
      "  Attempting uninstall: azureml-train-automl-client\n",
      "    Found existing installation: azureml-train-automl-client 1.48.0\n",
      "    Uninstalling azureml-train-automl-client-1.48.0:\n",
      "      Successfully uninstalled azureml-train-automl-client-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline-core\n",
      "    Found existing installation: azureml-pipeline-core 1.48.0\n",
      "    Uninstalling azureml-pipeline-core-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-core-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline-steps\n",
      "    Found existing installation: azureml-pipeline-steps 1.48.0\n",
      "    Uninstalling azureml-pipeline-steps-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-steps-1.48.0\n",
      "  Attempting uninstall: azureml-pipeline\n",
      "    Found existing installation: azureml-pipeline 1.48.0\n",
      "    Uninstalling azureml-pipeline-1.48.0:\n",
      "      Successfully uninstalled azureml-pipeline-1.48.0\n",
      "  Attempting uninstall: azureml-sdk\n",
      "    Found existing installation: azureml-sdk 1.48.0\n",
      "    Uninstalling azureml-sdk-1.48.0:\n",
      "      Successfully uninstalled azureml-sdk-1.48.0\n",
      "Successfully installed azure-mgmt-network-21.0.1 azureml-automl-core-1.53.0 azureml-core-1.53.0 azureml-dataprep-4.12.4 azureml-dataprep-rslex-2.19.5 azureml-dataset-runtime-1.53.0 azureml-pipeline-1.53.0 azureml-pipeline-core-1.53.0 azureml-pipeline-steps-1.53.0 azureml-sdk-1.53.0 azureml-telemetry-1.53.0 azureml-train-automl-client-1.53.0 azureml-train-core-1.53.0 azureml-train-restclients-hyperdrive-1.53.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azureml-sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e934f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n"
     ]
    }
   ],
   "source": [
    "#%%writefile {modules_dir}/aml_config.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "import json\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "#def create_ml_client(subscription_id: str, resource_group: str, workspace_name: str, tenant_id: str = None):\n",
    "def create_ml_client():\n",
    "    \"\"\"\n",
    "    Create an Azure Machine Learning workspace client.\n",
    "\n",
    "    This function attempts to create an Azure Machine Learning workspace client using the provided parameters. If it fails\n",
    "    to create a client, it generates a new configuration file with the provided parameters and tries again.\n",
    "\n",
    "    Parameters:\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group (str): Azure resource group name.\n",
    "        workspace_name (str): Azure Machine Learning workspace name.\n",
    "        tenant_id (str, optional): Azure Active Directory tenant ID. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        azureml.core.Workspace: An Azure Machine Learning workspace client.\n",
    "    \"\"\"\n",
    "    # Create an Azure CLI credential\n",
    "    credentials = AzureCliCredential(tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')\n",
    "    \n",
    "    try:\n",
    "        # Try to create the Azure Machine Learning workspace client using provided parameters\n",
    "        ml_client = Workspace.from_config(auth=credentials)\n",
    "    except Exception as ex:\n",
    "        print(\"An error occurred while creating the AML client:\", str(ex))\n",
    "        print(\"Creating a new configuration file...\")\n",
    "\n",
    "        # Define the workspace configuration based on the provided parameters\n",
    "        client_config = {\n",
    "            \"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group\": \"practice_resource\",\n",
    "            \"workspace_name\": \"practice_workspace\",\n",
    "        }\n",
    "\n",
    "        # Write the configuration to a JSON file\n",
    "        config_path = \"../config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            json.dump(client_config, fo)\n",
    "        \n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        ml_client = MLClient.from_config(credential=credentials, path=config_path)\n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        #ml_client = Workspace.from_config(path=config_path)\n",
    "    return ml_client\n",
    "   \n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "def get_compute(ml_client, compute_name:str, vm_size:str, min_instance:int, max_instances:int):\n",
    "    ml_client = create_ml_client()\n",
    "    # specify aml compute name.\n",
    "    cpu_compute_target = compute_name\n",
    "    \n",
    "    try:\n",
    "        cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "        print(f'Using existing compute target: {cpu_compute_target}')\n",
    "    except KeyError:\n",
    "        print(f\"Creating a new cpu compute target: {cpu_compute_target}...\")\n",
    "        cpu_cluster = AmlCompute(\n",
    "            name = cpu_compute_target,\n",
    "            size=vm_size,\n",
    "            min_nodes=min_instance,\n",
    "            max_nodes=max_instances\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).result()\n",
    "        \n",
    "    return cpu_compute_target, cpu_cluster   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7fc62e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLClient(credential=<azure.identity._credentials.azure_cli.AzureCliCredential object at 0x7f9ff7809a60>,\n",
       "         subscription_id=1ebe1808-a398-4ab0-b17c-1e3649ea39d5,\n",
       "         resource_group_name=practice_resource,\n",
       "         workspace_name=practice_workspace)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.create_ml_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0de69c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name.\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"STANDARD_E16S_V3\", min_instances=0, max_instances=4\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "623ce7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n",
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing compute target: cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "ml_client = aml.create_ml_client()\n",
    "cpu_compute_target, cpu_cluster = get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2a92b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_ingestion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_ingestion.py\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace, Experiment, Run, Dataset, Datastore\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace\n",
    "\n",
    "#def load_data(subscription_id: str, resource_group: str, workspace_name: str, data_name: str):\n",
    "    # Use DefaultAzureCredential to authenticate\n",
    "    #creds = DefaultAzureCredential()\n",
    "    \n",
    "    # Connect to the Azure Machine Learning workspace\n",
    "#    ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name)\n",
    "    \n",
    "    # Construct the data path using the workspace datastore\n",
    "#    datastore = ws.get_default_datastore()\n",
    "#    data_path = datastore.path(data_name)\n",
    "    \n",
    "#    log.info(f'The data path is: {data_path}')\n",
    "#    return data_path\n",
    "\n",
    "def load_data():\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    try:\n",
    "        # pipeline run\n",
    "        ws = run.experiment.workspace\n",
    "    except:\n",
    "        ws = Workspace.from_config()\n",
    "    # Make sure that 'ds' is a valid datastore name\n",
    "    #if ds is None:\n",
    "        #raise ValueError(\"The 'ds' parameter cannot be None.\")\n",
    "        \n",
    "    \n",
    "    #def_ds = Datastore.get(ws, ds)\n",
    "    def_ds = Datastore.get(ws, 'workspaceblobstore')\n",
    "    #load_data('workspaceblobstore', 'ai4i2020.csv')\n",
    "    raw_df = Dataset.Tabular.from_delimited_files([(def_ds,'ai4i2020.csv')]).to_pandas_dataframe()\n",
    "    log.info('Data Loaded')\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "def set_cwd_path(path: str):\n",
    "    os.chdir(path)\n",
    "    log.info(f'Current Directory set to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1ed6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace, Experiment, Run, Dataset, Datastore\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def load_data():\n",
    "    run = Run.get_context()\n",
    "\n",
    "    try:\n",
    "        # pipeline run\n",
    "        ws = run.experiment.workspace\n",
    "        def_ds = Datastore.get(ws, 'workspaceblobstore')\n",
    "        raw_df = Dataset.Tabular.from_delimited_files([(def_ds, 'ai4i2020.csv')]).to_pandas_dataframe()\n",
    "        mlflow.log_info('Data Loaded')\n",
    "        return raw_df  # Return the loaded DataFrame if successful\n",
    "    except Exception as e:\n",
    "        mlflow.log_info(f'Error loading data: {str(e)}')\n",
    "        return None  # Return None if an exception occurs\n",
    "\n",
    "def drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True, artifact_save_dir='artefacts'):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on Spearman correlation coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the dataset.\n",
    "    - corr_threshold: The threshold for correlation above which features will be dropped (default is 0.8).\n",
    "    - plot_heatmaps: Whether to plot heatmaps before and after dropping (default is True).\n",
    "    - artifact_save_dir: Directory to save the correlation heatmap plots (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the highly correlated features dropped.\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    if artifact_save_dir and not os.path.exists(artifact_save_dir):\n",
    "        os.makedirs(artifact_save_dir)\n",
    "    \n",
    "    # Calculate the correlation matrix (Spearman by default in pandas)\n",
    "    corr_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Plot the correlation heatmap before dropping\n",
    "        fig_before = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (Before Dropping)\")\n",
    "        sns_plot_before = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "            log.info(\"Correlation heatmap (Before Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_before.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Create a set to store the columns to drop\n",
    "    columns_to_drop = set()\n",
    "    \n",
    "    # Create a list to store the names of the dropped columns\n",
    "    dropped_columns = []\n",
    "    \n",
    "    # Iterate through the columns and identify highly correlated features\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            if col1 != col2 and abs(corr_matrix.loc[col1, col2]) >= corr_threshold:\n",
    "                # Check if col1 or col2 should be dropped based on their mean correlation\n",
    "                mean_corr_col1 = corr_matrix.loc[col1, :].drop(col1).abs().mean()\n",
    "                mean_corr_col2 = corr_matrix.loc[col2, :].drop(col2).abs().mean()\n",
    "                \n",
    "                if mean_corr_col1 > mean_corr_col2:\n",
    "                    columns_to_drop.add(col1)\n",
    "                    dropped_columns.append(col1)\n",
    "                else:\n",
    "                    columns_to_drop.add(col2)\n",
    "                    dropped_columns.append(col2)\n",
    "    \n",
    "    # Drop the highly correlated features from the DataFrame\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    if plot_heatmaps:\n",
    "        # Calculate the correlation matrix after dropping\n",
    "        corr_matrix_after_drop = df.corr(method='spearman')\n",
    "        \n",
    "        # Plot the correlation heatmap after dropping\n",
    "        fig_after = plt.figure(figsize=(8, 6))\n",
    "        plt.title(\"Correlation Heatmap (After Dropping)\")\n",
    "        sns_plot_after = sns.heatmap(corr_matrix_after_drop, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        if artifact_save_dir:\n",
    "            plt.savefig(os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "            log.info(\"Correlation heatmap (After Dropping): %s\", os.path.join(artifact_save_dir, \"correlation_heatmap_after.png\"))\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    log.info(\"Dropped columns: %s\", dropped_columns)\n",
    "\n",
    "def drop_high_cardinality_features(df, max_unique_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Drop high cardinality features (columns) from a DataFrame based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        max_unique_threshold (float): The maximum allowed fraction of unique values in a column (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with high cardinality columns dropped.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"Input DataFrame 'df' cannot be None.\")\n",
    "        \n",
    "    # Calculate the maximum number of allowed unique values for each column\n",
    "    max_unique_values = len(df) * max_unique_threshold\n",
    "    \n",
    "    # Identify and drop columns with unique values exceeding the threshold\n",
    "    high_cardinality_columns = [col for col in df.columns if df[col].nunique() > max_unique_values]\n",
    "    \n",
    "    # Log the names of the dropped columns\n",
    "    if high_cardinality_columns:\n",
    "        log.info(f\"Dropped high cardinality columns: {', '.join(high_cardinality_columns)}\")\n",
    "    \n",
    "    df_dropped = df.drop(columns=high_cardinality_columns)\n",
    "    \n",
    "    return df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9904d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.run:Could not load the run context. Logging offline\n",
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n",
      "INFO:azureml.data.datastore_client:<azureml.core.authentication.InteractiveLoginAuthentication object at 0x7f9ff6ecee80>\n",
      "INFO:__main__:Data Loaded\n",
      "INFO:__main__:Dropped high cardinality columns: UDI, Product ID\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>M</td>\n",
       "      <td>298.8</td>\n",
       "      <td>308.4</td>\n",
       "      <td>1604</td>\n",
       "      <td>29.5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>H</td>\n",
       "      <td>298.9</td>\n",
       "      <td>308.4</td>\n",
       "      <td>1632</td>\n",
       "      <td>31.8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>M</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1645</td>\n",
       "      <td>33.4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>H</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>48.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>M</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1500</td>\n",
       "      <td>40.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0       M                298.1                    308.6   \n",
       "1       L                298.2                    308.7   \n",
       "2       L                298.1                    308.5   \n",
       "3       L                298.2                    308.6   \n",
       "4       L                298.2                    308.7   \n",
       "...   ...                  ...                      ...   \n",
       "9995    M                298.8                    308.4   \n",
       "9996    H                298.9                    308.4   \n",
       "9997    M                299.0                    308.6   \n",
       "9998    H                299.0                    308.7   \n",
       "9999    M                299.0                    308.7   \n",
       "\n",
       "      Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Machine failure  \\\n",
       "0                       1551         42.8                0                0   \n",
       "1                       1408         46.3                3                0   \n",
       "2                       1498         49.4                5                0   \n",
       "3                       1433         39.5                7                0   \n",
       "4                       1408         40.0                9                0   \n",
       "...                      ...          ...              ...              ...   \n",
       "9995                    1604         29.5               14                0   \n",
       "9996                    1632         31.8               17                0   \n",
       "9997                    1645         33.4               22                0   \n",
       "9998                    1408         48.5               25                0   \n",
       "9999                    1500         40.2               30                0   \n",
       "\n",
       "      TWF  HDF  PWF  OSF  RNF  \n",
       "0       0    0    0    0    0  \n",
       "1       0    0    0    0    0  \n",
       "2       0    0    0    0    0  \n",
       "3       0    0    0    0    0  \n",
       "4       0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "9995    0    0    0    0    0  \n",
       "9996    0    0    0    0    0  \n",
       "9997    0    0    0    0    0  \n",
       "9998    0    0    0    0    0  \n",
       "9999    0    0    0    0    0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data()\n",
    "df = drop_high_cardinality_features(df, max_unique_threshold=0.9)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55131e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3f7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.run:Could not load the run context. Logging offline\n",
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n",
      "INFO:azureml.data.datastore_client:<azureml.core.authentication.InteractiveLoginAuthentication object at 0x7fa00aad1280>\n",
      "INFO:__main__:Data Loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>M24855</td>\n",
       "      <td>M</td>\n",
       "      <td>298.8</td>\n",
       "      <td>308.4</td>\n",
       "      <td>1604</td>\n",
       "      <td>29.5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>H39410</td>\n",
       "      <td>H</td>\n",
       "      <td>298.9</td>\n",
       "      <td>308.4</td>\n",
       "      <td>1632</td>\n",
       "      <td>31.8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>M24857</td>\n",
       "      <td>M</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1645</td>\n",
       "      <td>33.4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>H39412</td>\n",
       "      <td>H</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>48.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>M24859</td>\n",
       "      <td>M</td>\n",
       "      <td>299.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1500</td>\n",
       "      <td>40.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0         1     M14860    M                298.1                    308.6   \n",
       "1         2     L47181    L                298.2                    308.7   \n",
       "2         3     L47182    L                298.1                    308.5   \n",
       "3         4     L47183    L                298.2                    308.6   \n",
       "4         5     L47184    L                298.2                    308.7   \n",
       "...     ...        ...  ...                  ...                      ...   \n",
       "9995   9996     M24855    M                298.8                    308.4   \n",
       "9996   9997     H39410    H                298.9                    308.4   \n",
       "9997   9998     M24857    M                299.0                    308.6   \n",
       "9998   9999     H39412    H                299.0                    308.7   \n",
       "9999  10000     M24859    M                299.0                    308.7   \n",
       "\n",
       "      Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Machine failure  \\\n",
       "0                       1551         42.8                0                0   \n",
       "1                       1408         46.3                3                0   \n",
       "2                       1498         49.4                5                0   \n",
       "3                       1433         39.5                7                0   \n",
       "4                       1408         40.0                9                0   \n",
       "...                      ...          ...              ...              ...   \n",
       "9995                    1604         29.5               14                0   \n",
       "9996                    1632         31.8               17                0   \n",
       "9997                    1645         33.4               22                0   \n",
       "9998                    1408         48.5               25                0   \n",
       "9999                    1500         40.2               30                0   \n",
       "\n",
       "      TWF  HDF  PWF  OSF  RNF  \n",
       "0       0    0    0    0    0  \n",
       "1       0    0    0    0    0  \n",
       "2       0    0    0    0    0  \n",
       "3       0    0    0    0    0  \n",
       "4       0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "9995    0    0    0    0    0  \n",
       "9996    0    0    0    0    0  \n",
       "9997    0    0    0    0    0  \n",
       "9998    0    0    0    0    0  \n",
       "9999    0    0    0    0    0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace, Experiment, Run, Dataset, Datastore\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(ds:str, path:str):\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    try:\n",
    "        # pipeline run\n",
    "        ws = run.experiment.workspace\n",
    "    except:\n",
    "        ws = Workspace.from_config()\n",
    "    # Make sure that 'ds' is a valid datastore name\n",
    "    if ds is None:\n",
    "        raise ValueError(\"The 'ds' parameter cannot be None.\")\n",
    "        \n",
    "    \n",
    "    def_ds = Datastore.get(ws, ds)\n",
    "    raw_df = Dataset.Tabular.from_delimited_files([(def_ds, path)]).to_pandas_dataframe()\n",
    "    log.info('Data Loaded')\n",
    "    \n",
    "    return raw_df\n",
    "load_data('workspaceblobstore', 'ai4i2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "90ef445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use IMDS\n",
      "Found the config file in: ./config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ws = MLClient.from_config(DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1fad0",
   "metadata": {},
   "source": [
    "### Create custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f248a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: general_env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8.*\n",
    "  - pip=23.2.*\n",
    "  - pip:\n",
    "    - numpy==1.22.*\n",
    "    - mlflow==2.4.1\n",
    "    - azureml-mlflow==1.53.0\n",
    "    - azureml-core==1.53.*\n",
    "    - azureml-defaults==1.53.*\n",
    "    - scikit-learn==1.3.*\n",
    "    - azure-ai-ml==1.9.0\n",
    "    - requests==2.31.*\n",
    "    - azure-identity==1.14.0\n",
    "    - scipy==1.7.1\n",
    "    - pandas==1.4.4\n",
    "    - shap==0.42.1\n",
    "    - joblib==1.3.2\n",
    "    - seaborn==0.11.2\n",
    "    - matplotlib==3.4.*\n",
    "    - shapely==2.0.*\n",
    "    - scikit-optimize==0.9.*\n",
    "    - mldesigner==0.1.0b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3459c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "6d786bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/environment_register.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/environment_register.py\n",
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "#aml_config_dir = os.path.abspath('../modules')  # Use an absolute path\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(aml_config_dir):\n",
    "#    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "#\n",
    "## Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "custom_env_name = \"general_environment\"\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "env_docker_conda = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for classification and regression tasks\",\n",
    "    conda_file=\"conda.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.4.0\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3101d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "scripts_dir = \"./src\"\n",
    "os.makedirs(scripts_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c5dbfb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scripts_dir}/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d4db9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64c44466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "scripts_dir = \"./src\"\n",
    "os.makedirs(scripts_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8ee829ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_prep.yaml\n",
    "# <component>\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  input_ds:\n",
    "    type: uri_file    \n",
    "  ms_threshold:\n",
    "    type: number\n",
    "  corr_threshold:\n",
    "    type: number\n",
    "  plot_heatmaps:\n",
    "    type: boolean\n",
    "  max_unique_threshold:\n",
    "    type: number\n",
    "outputs:\n",
    "  output_data:  # Move 'output_data' to the 'inputs' section\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:general_environment:0.4.0\n",
    "command: >-\n",
    "  python data_preparation.py # Use the input defined above\n",
    "  --input_ds ${{inputs.input_ds}}\n",
    "  --ms_threshold ${{inputs.ms_threshold}}\n",
    "  --corr_threshold ${{inputs.corr_threshold}}\n",
    "  --plot_heatmaps ${{inputs.plot_heatmaps}}\n",
    "  --max_unique_threshold ${{inputs.max_unique_threshold}}\n",
    "  --output_data ${{outputs.output_data}}  # Reference 'output_data' as an input\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df325dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mldesigner\n",
      "  Downloading mldesigner-0.1.0b15-py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydash>=5.1.2 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (4.7.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /Users/ejenamvictor/opt/anaconda3/lib/python3.9/site-packages (from mldesigner) (6.0)\n",
      "Installing collected packages: mldesigner\n",
      "Successfully installed mldesigner-0.1.0b15\n"
     ]
    }
   ],
   "source": [
    "!pip install mldesigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9baf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "\n",
    "machine_ds = Input(\n",
    "    path='/Users/ejenamvictor/Desktop/project_CAS/ai4i2020.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a7e528",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/y7kf04rj02xbdjnpf7k716yw0000gn/T/ipykernel_88722/589384788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"load the data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     environment=dict(\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mconda_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"conda.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     ),\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from mldesigner import command_component, Input, Output\n",
    "\n",
    "\n",
    "@command_component(\n",
    "    name=\"prep_data\",\n",
    "    version=\"1\",\n",
    "    display_name=\"Prep Data\",\n",
    "    description=\"load the data\",\n",
    "    environment=dict(\n",
    "        conda_file=Path(__file__).parent / \"conda.yaml\",\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    ),\n",
    ")\n",
    "def prepare_data_component(\n",
    "    input_data: Input(type=\"uri_folder\"),\n",
    "    output_data: Output(type=\"uri_folder\"),\n",
    "):\n",
    "    df = drop_high_cardinality_features(df=raw_data, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "    \n",
    "    os.makedirs(args.output_data, exist_ok=True)\n",
    "    processed_data_path = os.path.join(args.output_data, 'processed_df.csv')\n",
    "    df.to_csv(processed_data_path, engine='pyarrow')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ca13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538800e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7260eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "62a240d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_preparation.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#modules_dir = os.path.join(current_dir, '..', 'modules')\n",
    "#sys.path.append(modules_dir)\n",
    "# Get the directory of the current script (assuming it's in the same directory)\n",
    "#script_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
    "\n",
    "# Add the directory containing \"data_prep.py\" to the Python path\n",
    "#sys.path.append(os.path.join(script_dir, 'modules'))\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "#aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(aml_config_dir):\n",
    "#    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "\n",
    "from data_prep import *\n",
    "from data_ingestion import *\n",
    "import argparse \n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "#def a custom argument type for a list of strings\n",
    "def list_of_strings(arg):\n",
    "    return arg.split(',')\n",
    "\n",
    "#if 'ipykernel' in sys.modules:\n",
    "    # Exclude IPython-specific arguments\n",
    "#    sys.argv = [arg for arg in sys.argv if not arg.endswith('ipykernel_launcher.py')]\n",
    "import aml_config as aml\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "def load_raw_data():\n",
    "    machine_data = ml_client.data.get(name='machine-failure', version='1')\n",
    "    data_path = machine_data.path  # Remove quotes around data_path variable\n",
    "    data = pd.read_csv(data_path)  # Use the data_path variable\n",
    "    return data  # Return the data frame\n",
    "\n",
    "def path_exists(path):\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(f\"Path {path} does not exist.\")\n",
    "   \n",
    "    \n",
    "def main():\n",
    "    # Check if running in IPython environment and exclude IPython-specific arguments\n",
    "#    if 'get_ipython' in globals():\n",
    "#        sys.argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
    "\n",
    "    #setup arg parser\n",
    "        \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_ds', dest='--input_ds', type=path_exists, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--ms_threshold', dest='ms_threshold', type=int, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--corr_threshold', dest='corr_threshold', type=float, help='The threshold for correlation above which features will be dropped')\n",
    "    parser.add_argument('--plot_heatmaps', dest='plot_heatmaps', type=bool, help='Whether to plot heatmaps before and after dropping (default is True)')\n",
    "    parser.add_argument('--max_unique_threshold', dest='max_unique_threshold', type=float, help='The maximum allowed fraction of unique values in a column (default is 0.9)')\n",
    "    parser.add_argument('--output_data', dest='output_data', type=str)\n",
    "        \n",
    "        # parse args\n",
    "    args = parser.parse_args()\n",
    "        # Start Logging\n",
    "        \n",
    "    mlflow.start_run()\n",
    "\n",
    "    #print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    #print(\"input data:\", args.data_path)\n",
    "\n",
    "    #raw_data = pd.read_csv(args.data_path, header=1, index_col=0)\n",
    "    #raw_data = pd.read_csv((Path(args.input_ds)))\n",
    "    raw_data = pd.read_csv(args.input_ds)\n",
    "        # Load in the data from synapse DataLake Storage\n",
    "        #raw_df = load_data(args.subscription_id, args.resource_group, args.workspace_name, args.data_name)\n",
    "        # Direct working directory to Artefact location\n",
    "        #set_cwd_path(args.plot_save_dir)\n",
    "        \n",
    "        #df = pd.read_csv(args.raw_data)\n",
    "        #if not os.path.exists(artifact_save_dir):\n",
    "        #os.makedirs(artifact_save_dir)\n",
    "        #if raw_data is None:\n",
    "        #    mlflow.log_info('Failed to load data.')  # Log an info message\n",
    "        #    mlflow.end_run()  # End the run\n",
    "        #    return \n",
    "            \n",
    "    raw_data = check_missing_values(raw_data)\n",
    "        # Direct working directory to Artefact location\n",
    "        #set_cwd_path(args.plot_save_dir)\n",
    "        \n",
    "            \n",
    "        #df = drop_high_cardinality_features(df=raw_data, max_unique_threshold=args.max_unique_threshold)\n",
    "        #df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "        #df = drop_highly_correlated_features(df, corr_threshold=args.corr_threshold, plot_heatmaps=args.plot_heatmaps)\n",
    "    \n",
    "        # Reset directory back to initial working directory\n",
    "        #set_cwd_path('..')\n",
    "    \n",
    "    mlflow.log_metric('Sample Size', raw_data.shape[0])\n",
    "    os.makedirs(args.output_data, exist_ok=True)\n",
    "    processed_data_path = os.path.join(args.output_data, 'processed_df.csv')\n",
    "    df.to_csv(processed_data_path, engine='pyarrow')\n",
    "    \n",
    "        #df.to_csv(os.path.join(args.output_data, 'processed_df.csv'), engine='pyarrow')\n",
    "    \n",
    "        # End Logging\n",
    "    #except Exception as e:\n",
    "    # Log information about the exception\n",
    "    #mlflow.log_param(\"exception_type\", type(e).__name__)\n",
    "    #mlflow.log_param(\"exception_message\", str(e))\n",
    "    #mlflow.log_param(\"exception_traceback\", traceback.format_exc())\n",
    " \n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "336556cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ejenamvictor/Desktop/project_CAS/modules\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(aml_config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8881dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/data_component_registration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/data_component_registration.py\n",
    "\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "#components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(current_directory, data_prep_yaml_file))\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "\n",
    "#parent_directory = 'modules/'  # Adjust this to your components directory\n",
    "\n",
    "#loaded_component_prep = load_component(source=current_dir + '/' + 'data_prep.yaml')\n",
    "\n",
    "# Loading the component from the yaml file\n",
    "loaded_component_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(loaded_component_prep)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c2b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ejenamvictor/Desktop/project_CAS/src/data_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "data_prep_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_prep_yaml_path = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, data_prep_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_path))\n",
    "\n",
    "print(data_prep_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c25ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d18f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_dir = \"./pipeline\"\n",
    "os.makedirs(pipeline_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "59ff0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/pipeline.py\n",
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "#aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from aml_config import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "\n",
    "cpu_compute_target, cpu_cluster = get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n",
    "\n",
    "parent_directory = '../modules/'  # Adjust this to your components directory\n",
    "\n",
    "data_prep = load_component(source=parent_directory + 'data_prep.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target\n",
    "    if (cpu_cluster)\n",
    "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"first pipeline\",\n",
    ")\n",
    "def classification_pipeline(\n",
    "    input_ds,\n",
    "    ms_threshold,\n",
    "    corr_threshold,\n",
    "    plot_heatmaps,\n",
    "    max_unique_threshold,\n",
    "    output_data,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep(\n",
    "        input_ds = input_ds,\n",
    "        ms_threshold = ms_threshold,\n",
    "        corr_threshold = corr_threshold,\n",
    "        plot_heatmaps = plot_heatmaps,\n",
    "        max_unique_threshold = max_unique_threshold\n",
    "    )\n",
    "    \n",
    "    #data_prep_job.outputs.output_data = Output(type='uri_folder', path=output_data, mode='rw_mount')\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.output_data,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f76570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d41f7c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./modules/pipeline_job_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {modules_dir}/pipeline_job_submission.py\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '..', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "#import pipeline as pi\n",
    "\n",
    "# Get the current working directory\n",
    "##current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "#components_relative_path = 'src'  # Adjust this to your components directory\n",
    "#data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "#aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "#component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "#data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "from aml_config import *\n",
    "from pipeline import *\n",
    "\n",
    "ml_client = create_ml_client()\n",
    "\n",
    "#machine_data = ml_client.data.get(name='machine-failure', version='1')\n",
    "#print(f'Data asset URI:{machine_data.path}')\n",
    "\n",
    "# Define input data and parameters\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = classification_pipeline(\n",
    "    input_ds=Input(type=\"uri_file\", path='/Users/ejenamvictor/Desktop/project_CAS/ai4i2020.csv'),\n",
    "    ms_threshold = 10,\n",
    "    corr_threshold = 0.8,\n",
    "    plot_heatmaps = True,\n",
    "    max_unique_threshold = 0.9,\n",
    "    output_data = 'processed_data'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#src_dir = os.path.join(current_dir, '.', 'modules')\n",
    "#sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabfa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5880353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0bd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ee3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70325a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a1d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc26b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d61ea98d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.workspace:Found the config file in: /Users/ejenamvictor/Desktop/project_CAS/config.json\n",
      "Found the config file in: ../config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while creating the AML client: 'AzureCliCredential' object has no attribute '_get_service_client'\n",
      "Creating a new configuration file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n",
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: plucky_coat_jzptvvnsnt\n",
      "Web View: https://ml.azure.com/runs/plucky_coat_jzptvvnsnt?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._internal.decorators:AzureCliCredential.get_token succeeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-09-30 13:33:20Z] Submitting 1 runs, first five are: 8f1602fb:f40e393c-9a1a-481f-a28c-48571b0f888b\n",
      "[2023-09-30 13:37:18Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: plucky_coat_jzptvvnsnt\n",
      "Web View: https://ml.azure.com/runs/plucky_coat_jzptvvnsnt?wsid=/subscriptions/1ebe1808-a398-4ab0-b17c-1e3649ea39d5/resourcegroups/practice_resource/workspaces/practice_workspace\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /data_prep_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"ukwest\",\n    \"location\": \"ukwest\",\n    \"time\": \"2023-09-30T13:37:18.037605Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/y7kf04rj02xbdjnpf7k716yw0000gn/T/ipykernel_98002/100178253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_prep_component\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mml_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/core/tracing/decorator.py\u001b[0m in \u001b[0;36mwrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mspan_impl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspan_impl_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/_telemetry/activity.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlog_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/operations/_job_operations.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPipelineChildJobError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         self._stream_logs_until_completion(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runs_operations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datastore_operations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requests_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azure/ai/ml/operations/_job_ops_helper.py\u001b[0m in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 raise JobException(\n\u001b[0m\u001b[1;32m    313\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Exception : \\n {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorTarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /data_prep_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"ukwest\",\n    \"location\": \"ukwest\",\n    \"time\": \"2023-09-30T13:37:18.037605Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "# Specify the directory containing the aml_config module\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "src_dir = os.path.join(current_dir, '.', 'modules')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "#aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(aml_config_dir):\n",
    "#    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "## Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b721e70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fac04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80362751",
   "metadata": {},
   "source": [
    "#%%writefile {components_dir}/data_prep.yaml\n",
    "\n",
    "#$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components_registration'  # Adjust this to your components directory\n",
    "data_prep_python_file = 'data_prep.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_prep_python_file))\n",
    "\n",
    "\n",
    "        \n",
    "# <component>\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  subscription_id:\n",
    "    type: string\n",
    "  resource_group:\n",
    "    type: string\n",
    "  workspace_name:\n",
    "    type: string\n",
    "  data_name:\n",
    "    type: string\n",
    "  plot_save_dir:\n",
    "    type: string\n",
    "  ms_threshold:\n",
    "    type: number\n",
    "  corr_threshold:\n",
    "    type: number\n",
    "  plot_heatmaps:\n",
    "    type: boolean\n",
    "  max_unique_threshold:\n",
    "    type: number\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ..\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python ${{inputs.data_prep_python_path}} \n",
    "  --subscription_id ${{inputs.subscription_id}} \n",
    "  --resource_group ${{inputs.resource_group}} \n",
    "  --workspace_name ${{inputs.workspace_name}}\n",
    "  --data_name ${{inputs.data_name}}\n",
    "  --plot_save_dir ${{inputs.plot_save_dir}}\n",
    "  --ms_threshold ${{inputs.ms_threshold}}\n",
    "  --corr_threshold ${{inputs.corr_threshold}}\n",
    "  --plot_heatmaps ${{inputs.plot_heatmaps}}\n",
    "  --max_unique_threshold ${{inputs.max_unique_threshold}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "component_register_dir = \"./component_registration\"\n",
    "os.makedirs(component_register_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {component_register_dir}/data_prep.py\n",
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "\n",
    "# Specify the directory containing the components\n",
    "#component_directory = os.path.abspath('./components')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(component_directory):\n",
    "#    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the components directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "\n",
    "# parent_directory = '../components/'\n",
    "# Loading the component from the yml file\n",
    "#loaded_component_prep = load_component(source=os.path.join(component_directory, \"data_prep.yaml\"))\n",
    "\n",
    "loaded_component_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep = ml_client.create_or_update(loaded_component_prep)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep.name} with Version {data_prep.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_prep_yaml_file = 'data_prep.yaml'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_yaml_path = os.path.abspath(os.path.join(component_directory, data_prep_yaml_file))\n",
    "\n",
    "# Check if the directories and files exist\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(component_directory):\n",
    "    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(data_prep_yaml_path):\n",
    "    print(f\"File '{data_prep_yaml_path}' does not exist. Please check the file path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Now, you have the absolute paths\n",
    "print(\"Absolute path to aml_config directory:\", aml_config_dir)\n",
    "print(\"Absolute path to components directory:\", component_directory)\n",
    "print(\"Absolute path to data_prep.yaml file:\", data_prep_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output, load_component\n",
    "import os\n",
    "import mlflow\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'components'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_preparation_py_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "# Specify the directory containing the components\n",
    "#component_directory = os.path.abspath('./components')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "#if not os.path.exists(component_directory):\n",
    "#    print(f\"Directory '{component_directory}' does not exist. Please check the directory path.\")\n",
    "#    sys.exit(1)\n",
    "\n",
    "# Add the components directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "# Loading the component from the yml file\n",
    "data_prep = load_component(source=data_prep_yaml_path)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "#sys.path.insert(0, component_directory)\n",
    "\n",
    "\n",
    "cpu_compute_target, cpu_cluster = aml.get_compute(ml_client, compute_name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target\n",
    "    if (cpu_cluster)\n",
    "    else \"serverless\",  # \"serverless\" value runs pipeline on serverless compute\n",
    "    description=\"first pipeline\",\n",
    ")\n",
    "def classification_pipeline(\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    workspace_name,\n",
    "    data_name,\n",
    "    plot_save_dir,\n",
    "    ms_threshold,\n",
    "    corr_threshold,\n",
    "    plot_heatmaps,\n",
    "    #columns_to_drop,\n",
    "    max_unique_threshold,\n",
    "    data_preparation_py_path\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep(\n",
    "        subscription_id = subscription_id,\n",
    "        resource_group = resource_group,\n",
    "        workspace_name = workspace_name,\n",
    "        data_name = data_name,\n",
    "        plot_save_dir = plot_save_dir,\n",
    "        ms_threshold = ms_threshold,\n",
    "        corr_threshold = corr_threshold,\n",
    "        plot_heatmaps = plot_heatmaps,\n",
    "        #columns_to_drop = columns_to_drop,\n",
    "        max_unique_threshold = max_unique_threshold,\n",
    "        data_preparation_py_path = data_preparation_py_path\n",
    "        \n",
    "    )\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.output_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "# Now, you have the absolute paths\n",
    "print(\"Absolute path to aml_config directory:\", aml_config_dir)\n",
    "print(\"Absolute path to components directory:\", component_directory)\n",
    "print(\"Absolute path to data_prep.yaml file:\", data_prep_python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative paths to your directories and files\n",
    "#aml_config_relative_path = 'modules'  # Adjust this to your aml_config directory\n",
    "components_relative_path = 'src'  # Adjust this to your components directory\n",
    "data_preparation_py_path = 'data_preparation.py'  # Adjust this to your data_prep.yaml file\n",
    "\n",
    "# Construct the absolute paths\n",
    "aml_config_dir = os.path.abspath(os.path.join(current_directory, aml_config_relative_path))\n",
    "component_directory = os.path.abspath(os.path.join(current_directory, components_relative_path))\n",
    "data_prep_python_path = os.path.abspath(os.path.join(component_directory, data_preparation_py_path))\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = classification_pipeline(\n",
    "    subscription_id = \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "    resource_group = \"practice_resource\",\n",
    "    workspace_name = \"practice_workspace\",\n",
    "    data_name = 'ai4i2020.csv',\n",
    "    plot_save_dir = 'plots',\n",
    "    ms_threshold = 10,\n",
    "    corr_threshold = 0.8,\n",
    "    plot_heatmaps = True,\n",
    "    #columns_to_drop = columns_to_drop,\n",
    "    max_unique_threshold = 0.9, \n",
    "    data_preparation_py_path = data_preparation_py_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02cb41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "# Specify the directory containing the aml_config module\n",
    "aml_config_dir = os.path.abspath('./modules')  # Use an absolute path\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(aml_config_dir):\n",
    "    print(f\"Directory '{aml_config_dir}' does not exist. Please check the directory path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add the aml_config directory to sys.path\n",
    "sys.path.insert(0, aml_config_dir)\n",
    "\n",
    "import aml_config as aml \n",
    "\n",
    "ml_client = aml.create_ml_client()\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"data_prep_component\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de5384",
   "metadata": {},
   "source": [
    "\"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group\": \"practice_resource\",\n",
    "            \"workspace_name\": \"practice_workspace\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106467a",
   "metadata": {},
   "source": [
    "df = dp.drop_high_cardinality_features(df, max_unique_threshold=0.9)\n",
    "df = dp.drop_highly_correlated_features(df, corr_threshold=0.8, plot_heatmaps=True)\n",
    "df = dp.replace_missing_values(df, ms_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {scripts_dir}/model_training.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/modules\")\n",
    "from data_prep import *\n",
    "from data_ingestion import *\n",
    "import argpase \n",
    "import mlflow\n",
    "\n",
    "#def a custom argument type for a list of strings\n",
    "def list_of_strings(arg):\n",
    "    return arg.split(',')\n",
    "\n",
    "def load_data(subscription_id:str, resource_group_name:str, workspace_name:str, data_path:str)\n",
    "def main():\n",
    "    #setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #add arguments\n",
    "    parser.add_argument('--subscription_id', dest='subscription_id',\n",
    "                       type=str, help=\"Azure subscription id\")\n",
    "    parser.add_argument('--resource_group', dest='resource_group',\n",
    "                       type=str, help=\"resource group name\")\n",
    "    parser.add_argument('--workspace_name', dest='workspace_name',\n",
    "                       type=str, help=\"workspace_name\")\n",
    "    parser.add_argument('--data_name', dest='data_name',\n",
    "                       type=str, help=\"data_path\")\n",
    "    parser.add_argument('--plot_save_dir', dest='plot_save_dir'\n",
    "                       type=str, help='Name of Parent Directory to store pipeline artefacts')\n",
    "    parser.add_argument('--ms_threshold', dest='ms_threshold',\n",
    "                       type=int, help='Threshold to switch between interpolation and iterative imputer')\n",
    "    parser.add_argument('--corr_threshold', dest='corr_threshold',\n",
    "                       type=float, help='The threshold for correlation above which features will be dropped')\n",
    "    parser.add_argument('--plot_heatmaps', dest='plot_heatmaps',\n",
    "                       type=bool, help='Whether to plot heatmaps before and after dropping (default is True)')\n",
    "    parser.add_argument('columns_to_drop', dest='columns_to_drop', \n",
    "                       type=list_of_strings, help='Single column name or a list of column names to be dropped')\n",
    "    parser.add_argument('--max_unique_threshold', dest='max_unique_threshold',\n",
    "                       type=float, help='The maximum allowed fraction of unique values in a column (default is 0.9)')\n",
    "    parser.add_argument('--output_data', dest='output_data',\n",
    "                       type=str)\n",
    "    \n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    # Load in the data from synapse DataLake Storage\n",
    "    raw_df = load_data(args.subscription_id, args.resource_group_name, args.workspace_name, args.data_name)\n",
    "    # Direct working directory to Artefact location\n",
    "    set_cwd_path(args.plot_save_dir)\n",
    "    \n",
    "    if args.training_run == 'True':\n",
    "        training_run = True\n",
    "    elif args.training_run == 'False':\n",
    "        training_run = False\n",
    "    else:\n",
    "        raise ValueError('Training Run Parameter must be \"True\" or \"False\"')\n",
    "        \n",
    "    df = check_missing_values(raw_df)\n",
    "    df = drop_high_cardinality_features(df, max_unique_threshold=args.max_unique_threshold)\n",
    "    df = replace_missing_values(df, ms_threshold=args.ms_threshold)\n",
    "    df = drop_highly_correlated_features(df, corr_threshold=args.threshold, plot_heatmaps=True)\n",
    "    \n",
    "    # Reset directory back to initial working directory\n",
    "    set_cwd_path('..')\n",
    "    \n",
    "    mlflow.log_metric('Sample Size', df.shape[0])\n",
    "    \n",
    "    df.to_parquet(os.path.join(args.output_data, 'processed_df.parquet'), engine='pyarrow')\n",
    "    \n",
    "    # End Logging\n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f55348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sd.custom_train_test_split(df, target_column, test_size=0.2, random_state=None, time_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {components_dir}/data_prep.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17190ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ca15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee95ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d2233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <component>\n",
    "name: data_prep\n",
    "display_name: data_preparation\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  subscription_id:\n",
    "    type: string\n",
    "  resource_group:\n",
    "    type: string\n",
    "  workspace_name:\n",
    "    type: string\n",
    "  data_name:\n",
    "    type: string\n",
    "  plot_save_dir:\n",
    "    type: string\n",
    "  ms_threshold:\n",
    "    type: number\n",
    "  corr_threshold:\n",
    "    type: number\n",
    "  plot_heatmaps:\n",
    "    type: boolean\n",
    "  columns_to_drop:\n",
    "    type: string\n",
    "  max_unique_threshold:\n",
    "    type: number\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python src/data_preparation.py \n",
    "  --subscription_id ${{inputs.subscription_id}} \n",
    "  --resource_group ${{inputs.resource_group}} \n",
    "  --workspace_name ${{inputs.workspace_name}}\n",
    "  --data_name ${{inputs.data_name}}\n",
    "  --plot_save_dir ${{inputs.plot_save_dir}}\n",
    "  --ms_threshold ${{inputs.ms_threshold}}\n",
    "  --corr_threshold ${{inputs.corr_threshold}}\n",
    "  --plot_heatmaps ${{inputs.plot_heatmaps}}\n",
    "  --columns_to_drop ${{inputs.columns_to_drop}}\n",
    "  --max_unique_threshold ${{inputs.max_unique_threshold}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f006f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile {modules_dir}/aml_config.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "import json\n",
    "\n",
    "#def create_ml_client(subscription_id: str, resource_group: str, workspace_name: str, tenant_id: str = None):\n",
    "def create_ml_client():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create an Azure Machine Learning workspace client.\n",
    "\n",
    "    This function attempts to create an Azure Machine Learning workspace client using the provided parameters. If it fails\n",
    "    to create a client, it generates a new configuration file with the provided parameters and tries again.\n",
    "\n",
    "    Parameters:\n",
    "        subscription_id (str): Azure subscription ID.\n",
    "        resource_group (str): Azure resource group name.\n",
    "        workspace_name (str): Azure Machine Learning workspace name.\n",
    "        tenant_id (str, optional): Azure Active Directory tenant ID. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        azureml.core.Workspace: An Azure Machine Learning workspace client.\n",
    "    \"\"\"\n",
    "    # Create an Azure CLI credential\n",
    "    credentials = AzureCliCredential(tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')\n",
    "    \n",
    "    try:\n",
    "        # Try to create the Azure Machine Learning workspace client using provided parameters\n",
    "        ml_client = Workspace.from_config(auth=credentials)\n",
    "    except Exception as ex:\n",
    "        print(\"An error occurred while creating the AML client:\", str(ex))\n",
    "        print(\"Creating a new configuration file...\")\n",
    "\n",
    "        # Define the workspace configuration based on the provided parameters\n",
    "        client_config = {\n",
    "            \"subscription_id\": \"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\",\n",
    "            \"resource_group_name\": \"victor_resource\",\n",
    "            \"workspace_name\": \"victor_workspace\",\n",
    "        }\n",
    "\n",
    "        # Write the configuration to a JSON file\n",
    "        config_path = \"../project_CAS/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            json.dump(client_config, fo)\n",
    "        \n",
    "        # Try to create the Azure Machine Learning workspace client again\n",
    "        ml_client = Workspace.from_config(path=config_path)\n",
    "    \n",
    "    return ml_client\n",
    "\n",
    "def get_compute(ml_client, name:str, vm_size:str, min_instance:int, max_instances:int):\n",
    "    # specify aml compute name.\n",
    "    cpu_compute_target = name\n",
    "    \n",
    "    try:\n",
    "        cpu_cluster = ml_client.compute_targets[cpu_compute_target]\n",
    "        print(f'Using existing compute target: {cpu_compute_target}')\n",
    "    except KeyError:\n",
    "        print(f\"Creating a new cpu compute target: {cpu_compute_target}...\")\n",
    "        compute_config = AmlCompute.provisioning_configuration(\n",
    "            vm_size=vm_size,\n",
    "            min_nodes=min_instance,\n",
    "            max_nodes=max_instances\n",
    "        )\n",
    "        cpu_cluster = AmlCompute.create(ml_client, name=cpu_compute_target, provisioning_configuration=compute_config)\n",
    "        cpu_cluster.wait_for_completion(show_output=True)\n",
    "        \n",
    "    return cpu_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cadb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"./modules\")\n",
    "import aml_config as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc89f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = ac.create_ml_client(subscription_id=\"1ebe1808-a398-4ab0-b17c-1e3649ea39d5\", resource_group=\"victor_resource\", workspace_name=\"victor_workspace\", tenant_id='6aa8da55-4c6f-496e-8fc1-de0f7819b03b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.get_compute(ml_client, name=\"cpu-cluster\", vm_size=\"STANDARD_E16S_V3\", min_instance=0, max_instances=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8.*\n",
    "  - pip=23.2.*\n",
    "  - pip:\n",
    "    - numpy==1.22.*\n",
    "    - mlflow== 2.4.1\n",
    "    - azureml-core==1.53.*\n",
    "    - azureml-defaults==1.53.*\n",
    "    - mlflow==2.6.*\n",
    "    - scikit-learn==1.3.*\n",
    "    - azure-ai-ml==1.9.0\n",
    "    - requests==2.31.*\n",
    "    - azure-identity==1.14.0\n",
    "    - scipy==1.7.1\n",
    "    - pandas==1.4.4\n",
    "    - shap==0.42.1\n",
    "    - joblib==1.3.2\n",
    "    - seaborn==0.11.2\n",
    "    - matplotlib==3.4.*\n",
    "    - shapely==2.0.*\n",
    "    - scikit-optimize==0.9.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
